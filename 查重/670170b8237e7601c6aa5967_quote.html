
    
<!DOCTYPE html>
<html lang="zh-hans">

<meta http-equiv="X-UA-Compatible" content="IE=7" />

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="author" content="Turnitin, LLC" />
    <meta name="keywords" content="" /> 
    <meta name="description" content="" />
<title>TurnitinUK - 原创性报告 - MaxPix: detecting GAN-generated images by emphasizing local maxima_1 </title>

<base href="http://www.turnitinuk.com">
<style type="text/css">
body {
    color: #333;
    background: #C0C7CC;
    padding: 0;
    border: 0;
    font: 13px Verdana, arial, sans-serif;
    margin: 0;
}

form {
    padding: 0;
    margin: 0;
}

body#display {
}

body#bodysource {
    width: 520px;
    background: #F0F4FA;
}

p {
    padding: 10px 18px;
    margin: 0;
}

img {
    border: 0;
    padding: 0;
}

div {
    padding: 0;
    border: 0;
}

iframe {
    border: 0;
    margin: 0;
    padding: 0;
}

strong {
    font-weight: bold;
}

ul {
    padding: 0;
    margin: 0;
    list-style-type: none;
    font-size: 13px;
}

ul li {
    padding: 0;
    margin: 0;
    line-height: 16px;
}

#index span#exclude {
    margin: 0 50px 0 23px;
}

#index a {
    font-size: 11px;
    padding: 0 8px;
}

#index select {
    font-size: 12px;
    border: 1px solid #888;
}

#index input.small {
    margin: 0 0 0 5px;
    width: 30px;
    color: #D10A0A;
    font-weight: bold;
    font-size: 13px;
    border: 1px solid #888;
    vertical-align: baseline;
}

div.links {
    width: 85%;
    margin: 0 auto;
    border-left: 1px solid #888;
    border-right: 1px solid #888;
    padding-top: 8px;
    background: #E8EEF7;
    text-align: left;
}

.links div {
    padding: 5px 13px 10px 20px;
    border-bottom: 1px dotted #888;
}

.links div p {
    padding: 2px 0 0 40px;
}

div#body {
    line-height: 17px;
    width: 85%;
    margin: auto;
    padding: 20px 0;
    background: #fff;
    border-bottom: 1px solid #888;
    border-right: 1px solid #888;
    border-left: 1px solid #888;
    text-align: left;
}

#body p {
    color: #000;
    padding-top: 10 0;
    margin: 0 40px;
}

#actions {
    display: none;
}

a.exclude {
    float: right;
    margin: 0;
    padding: 0;
    position: relative;
    bottom: 20px;
}

/*= SMALL MATCHES POPUP
=== === === === === === === === === === === === === === === === === === === === === === === === === === === === === === */
div#small_matches_prefs {
    visibility: hidden;
    position: absolute;
    top: 0;
    left: 400px;
    background-color: #FFF;
    border: 1px solid #999;
    text-align: right;
}

div#small_matches_prefs p {
    padding: 7px;
}

div#small_matches_prefs li {
    padding: 10px 40px 10px 0;
    border-bottom: 1px solid #999;
    cursor: pointer;
    text-align: left;
}

div#small_matches_prefs li.selected {
    background-color: #87A3C0;
}

div#small_matches_prefs li input {
    text-align: center;
    border: 1px solid #999;
}

div#small_matches_prefs li.disabled input {
    color: #878787;
    background-color: #E6E5E6;
}

div#small_matches_prefs ul label {
    width: 100px;
    text-align: right;
    display: inline-block;
    margin-left: 10px;
    margin-right: 10px;
}
/*= GENERAL
=== === === === === === === === === === === === === === === === === === === === === === === === === === === === === === */

body #top_bar {
    display: none !important;
}

body #index #exclude,
#download_button,
#print_button,
#index .right {
    display: inline-block;
}

body #index {
    width: 85%;
    margin-left: auto;
    margin-right: auto;
    border: 1px solid #999;
    background: #ececec url(new_dynamic/images/22bd7a01a025b8de122259e42762f0a7cb_ug_toolbar_bg.gif) repeat-x center left;
}

#toolbar_wrapper {
    padding-left: 45px;
}

body #top {
    width: 85%;
    background-color: #FFF;
    margin-left: auto;
    margin-right: auto;
    border: 1px solid #999;
    border-bottom: none;
    height: 210px;
}

body #content {
    padding: 10px 60px;
}

body div#prefs {
    display: none;
}

body #top h1 {
    font-size: 20px;
    font-weight: normal;
}

body #top h1 strong {
    font-weight: normal;
}

body #top h1 em {
    font-style: normal;
}


/*body #top h2 {*/
/*    font-size: 20px;*/
/*    font-weight: normal;*/
/*}*/

/*body #top h2 strong {*/
/*    font-weight: normal;*/
/*}*/

/*body #top h2 em {*/
/*    font-style: normal;*/
/*}*/


body #top h2 {
    font-size: 16px;
    font-weight: normal;
}

body #top h2 strong {
    font-weight: normal;
}

body #top h2 em {
    font-style: normal;
}


body #top_body li { /*Paper info li*/
    padding: 0;
    margin: 0px 0px 2px 0px;
    font-size: 10px;
}

#top_body #print_wrapper {
    float: left;
    width: 50%;
}

#top_body .similarity_print_wrapper {
    width: 45%;
    min-width: 283px;
}

#top_body .similarity_box { /*Similarity Box w/ Similarity by Source */
    float: right;
    border: solid 1px #666;
    margin-top: 60px;
    min-width: 350px;
}

#top_body .similarity_box .overall_similarity {
    float: left;
    border-right: solid 1px #666;
}

#top_body .similarity_box .overall_similarity .color_box {
    font-size: 14px;
    min-width: 140px;
}

#top_body .color_box.green {
    background-color: green;
}

#top_body .color_box.blue {
    background-color: blue;
}

#top_body .color_box.yellow {
    background-color: yellow;
}

#top_body .color_box.orange {
    background-color: orange;
}

#top_body .color_box.red {
    background-color: red;
}

#top_body .similarity_box .overall_similarity .similarity_title {
    font-size: 13px;
    font-weight: normal;
    padding: 5px 5px 5px;
    text-align: center;
}

#top_body .similarity_box .overall_similarity .similarity_percent {
    font-size: 25px;
    font-family: georgia, times, serif;
    padding: 5px 0px 15px;
    text-align: center;
}

#top_body .similarity_box .overall_similarity a {
    display: none;
}

#top_body .similarity_box .similarity_by_source {
    float: right;
    font-size: 10px;
}

#top_body .similarity_box .similarity_by_source .similarity_title {
    padding: 6px 0px 0px 10px;
    font-weight: bold;
    text-align: left;
}

#top_body .similarity_box .similarity_by_source dl {
    padding-left: 10px;
    margin: 14px 7px 0px 0px;
}

#top_body .similarity_box .similarity_by_source dt {
    float: left;
    width: 160px;
}

#top_body .similarity_box .similarity_by_source dd {
    float: left;
    margin: 0px;
}


#index span#exclude {
    margin: 0 50px 0 23px;
}

#index a {
    font-size: 11px;
    padding: 0 8px;
}

#index select {
    font-size: 12px;
    border: 1px solid #888;
}

#index input.small {
    margin: 0 0 0 5px;
    width: 30px;
    color: #D10A0A;
    font-weight: bold;
    font-size: 13px;
    border: 1px solid #888;
    vertical-align: baseline;
}

div.links {
    width: 85%;
    margin: 0 auto;
    border-left: 1px solid #888;
    border-right: 1px solid #888;
    padding-top: 8px;
    background: #E8EEF7;
    text-align: left;
}

.links div {
    padding: 5px 13px 10px 20px;
    border-bottom: 1px dotted #888;
}

.links div p {
    padding: 2px 0 0 40px;
}

div#body {
    line-height: 17px;
    width: 85%;
    margin: auto;
    padding: 20px 0;
    background: #fff;
    border-bottom: 1px solid #888;
    border-right: 1px solid #888;
    border-left: 1px solid #888;
    text-align: left;
}

#body p {
    color: #000;
    padding-top: 10 0;
    margin: 0 40px;
}

#actions {
    display: none;
}

button.exclude {
    float: right;
    margin: 0;
    padding: 0;
    border: none;
}

#small_matches_prefs {
    display: none;
}

/*
Copyright (c) 2009, Yahoo! Inc. All rights reserved.
Code licensed under the BSD License:
http://developer.yahoo.net/yui/license.txt
version: 2.7.0
*/
.yui-button{display:-moz-inline-box;display:inline-block;vertical-align:text-bottom;}.yui-button .first-child{display:block;*display:inline-block;}.yui-button button,.yui-button a{display:block;*display:inline-block;border:none;margin:0;}.yui-button button{background-color:transparent;*overflow:visible;cursor:pointer;}.yui-button a{text-decoration:none;}.yui-skin-sam .yui-button{border-width:1px 0;border-style:solid;border-color:#808080;background:url(../images/yui270/build/assets/skins/sam/96b257a32a932f7739d7dab52b38ee8fcb_sprite.png) repeat-x 0 0;margin:auto .25em;}.yui-skin-sam .yui-button .first-child{border-width:0 1px;border-style:solid;border-color:#808080;margin:0 -1px;_margin:0;}.yui-skin-sam .yui-button button,.yui-skin-sam .yui-button a{padding:0 10px;font-size:93%;line-height:2;*line-height:1.7;min-height:2em;*min-height:auto;color:#000;}.yui-skin-sam .yui-button a{*line-height:1.875;*padding-bottom:1px;}.yui-skin-sam .yui-split-button button,.yui-skin-sam .yui-menu-button button{padding-right:20px;background-position:right center;background-repeat:no-repeat;}.yui-skin-sam .yui-menu-button button{background-image:url(yui270/build/button/assets/skins/sam/6305efb37fa05af65c79b58b9d4c1b03cb_menu-button-arrow.png);}.yui-skin-sam .yui-split-button button{background-image:url(yui270/build/button/assets/skins/sam/ced974d5c685e5dfa0a37b824a6b5d48cb_split-button-arrow.png);}.yui-skin-sam .yui-button-focus{border-color:#7D98B8;background-position:0 -1300px;}.yui-skin-sam .yui-button-focus .first-child{border-color:#7D98B8;}.yui-skin-sam .yui-button-focus button,.yui-skin-sam .yui-button-focus a{color:#000;}.yui-skin-sam .yui-split-button-focus button{background-image:url(yui270/build/button/assets/skins/sam/36e66540d2feba76b8991e18b76fe93bcb_split-button-arrow-focus.png);}.yui-skin-sam .yui-button-hover{border-color:#7D98B8;background-position:0 -1300px;}.yui-skin-sam .yui-button-hover .first-child{border-color:#7D98B8;}.yui-skin-sam .yui-button-hover button,.yui-skin-sam .yui-button-hover a{color:#000;}.yui-skin-sam .yui-split-button-hover button{background-image:url(yui270/build/button/assets/skins/sam/36e66540d2feba76b8991e18b76fe93bcb_split-button-arrow-hover.png);}.yui-skin-sam .yui-button-active{border-color:#7D98B8;background-position:0 -1700px;}.yui-skin-sam .yui-button-active .first-child{border-color:#7D98B8;}.yui-skin-sam .yui-button-active button,.yui-skin-sam .yui-button-active a{color:#000;}.yui-skin-sam .yui-split-button-activeoption{border-color:#808080;background-position:0 0;}.yui-skin-sam .yui-split-button-activeoption .first-child{border-color:#808080;}.yui-skin-sam .yui-split-button-activeoption button{background-image:url(yui270/build/button/assets/skins/sam/890272b241c1d8a0db3ce5680b71fab0cb_split-button-arrow-active.png);}.yui-skin-sam .yui-radio-button-checked,.yui-skin-sam .yui-checkbox-button-checked{border-color:#304369;background-position:0 -1400px;}.yui-skin-sam .yui-radio-button-checked .first-child,.yui-skin-sam .yui-checkbox-button-checked .first-child{border-color:#304369;}.yui-skin-sam .yui-radio-button-checked button,.yui-skin-sam .yui-checkbox-button-checked button{color:#fff;}.yui-skin-sam .yui-button-disabled{border-color:#ccc;background-position:0 -1500px;}.yui-skin-sam .yui-button-disabled .first-child{border-color:#ccc;}.yui-skin-sam .yui-button-disabled button,.yui-skin-sam .yui-button-disabled a{color:#A6A6A6;cursor:default;}.yui-skin-sam .yui-menu-button-disabled button{background-image:url(yui270/build/button/assets/skins/sam/4df7235ca027f2546b2a216e59f81fb0cb_menu-button-arrow-disabled.png);}.yui-skin-sam .yui-split-button-disabled button{background-image:url(yui270/build/button/assets/skins/sam/db73dce6da2f5c5f02399c93488ce69ecb_split-button-arrow-disabled.png);}

</style>



</head>

<body onload="">



<link rel="stylesheet" type="text/css" href="/r/build/css/tii/88ee4ccd3555f2b759921fb5d58d83e5cb_container.css" media="all" />




<script type="text/javascript" src="/r/build/js/tii/8b608684a5f4aec1b540987c93498c01cb_tii_anonymous_marking.js"></script>




<script type="text/javascript">

function initAnonymousMarking () {
    // initialize panel.  
    var config = {
            zindex: 4,
            underlay: 'none',
            modal: true,
            visible: false,
            draggable: false,
            close: false,
            fixedcenter: true
    };
    if ($('disable_anonymous_marking')) {
        disableAnonymousMarkingPanel = new IP.widget.Panel($('disable_anonymous_marking'), config);
        if($D.hasClass('disable_anonymous_marking', 'app')) {
            disableAnonymousMarkingPanel.center = function () {
                var nViewportOffset = 20,
                    elementWidth = this.element.offsetWidth,
                    elementHeight = this.element.offsetHeight,
                    viewPortWidth = $D.getViewportWidth(),
                    viewPortHeight = $D.getViewportHeight(),
                    x,
                    y;

                if (elementWidth < viewPortWidth) {
                    x = (viewPortWidth / 2) - (elementWidth / 2) + $D.getDocumentScrollLeft();
                } else {
                    x = nViewportOffset + $D.getDocumentScrollLeft();
                }

				if (browser == 'Internet Explorer') {
					x = 0;
				}
                y = 2 + $D.getDocumentScrollTop();

                this.cfg.setProperty("xy", [parseInt(x, 10), parseInt(y, 10)]);
                this.cfg.refireEvent("iframe");
            };
        }
        disableAnonymousMarkingPanel.render(document.body);
        disableAnonymousMarkingPanel.hideEvent.subscribe(function () { $('anonymous_error').innerHTML = ''; }, false);
        disableAnonymousMarkingPanel.hide();
        Element.show($('disable_anonymous_marking'));
    }
}

function disableAM (data) {
    $('anonymous_title').innerHTML = data.title;
    document.disable_anonymous_marking_form.objectid.value = data.oid;
    disableAnonymousMarkingPanel.show();
}

function checkDisableAM () {
    var form = document.disable_anonymous_marking_form;
    if (form.reason.value.length <= 5) {
        $('anonymous_error').innerHTML = "请提供关闭匿名标记的原因。您的理由必须超过 10 个字符长度。";
    }
    else {
        form.submit();
    }
    return;
}

YAHOO.util.Event.onDOMReady(initAnonymousMarking);

</script>

<div id="disable_anonymous_marking" class="app" style="display: none;">
<div class="anonymous_frames">
<form method="post" name="disable_anonymous_marking_form">
    <input type="hidden" name="objectid" value=""/>
    <input type="hidden" name="disable_anonymous_marking" value="1"/> 
    <div class="anonymous_header">
    	<h1>关闭匿名标记</h1>
		<p>请说明关闭匿名标记的原因： <span id="anonymous_title"></span><br />
			<strong>警告: 管理员可以存取此资讯。此设定是永久性的。</strong>
        
            </p>
        
    </div>
    <div class="anonymous_body">
        <textarea name="reason" cols="30" rows="3"></textarea>
        <p id="anonymous_error"></p>
    </div>
    <div class="anonymous_footer">
		<div class="anonymous_footer_buttons">
            <span class="submit_form_button"><input type="button" onClick="checkDisableAM();" value="提交"></span><br>
        	<span class="submit_form_button"><input type="button" value="取消" onClick="disableAnonymousMarkingPanel.hide();"></span>
		</div>
    </div>
</form>
</div>
</div>
<div id="actions">
<p>这是您报告的印表版的检视。请点击 "打印" 以继续或 "结束" 以关闭视窗。</p>
<script type="text/javascript" language="javascript">
	var browserName=navigator.appName;
	var browserVer=parseInt(navigator.appVersion);
	if ((navigator.appVersion.indexOf("Mac")!=-1) && (browserName == "Microsoft Internet Explorer")) {
		document.write('<span class="AR10">键入 COMMAND-P 开始打印。</span><br><br>');
	} else {
		document.write('<button onclick="window.print();">打印</button>&nbsp;&nbsp;');
	}
</script>
<button onclick="window.close();">完成</button>
</div>


<!-- ########################### Preferences pop-up ##########################--> 

<div name="top" id="header">

<div id="prefs" role="dialog" style="display:none" aria-labelledby="prefs_link" aria-describedby="prefs_link" aria-owns="prefs_link">
<div class="padding">
<form name="prefs_form" method="post" accept-charset="utf-8">
<script type="text/javascript" language="javascript">
function savePrefs(){
	if (document.prefs_form.changed.value == 1){
		document.prefs_form.submit();
	}else{
        hidePrefsPane();
	}
}

function handlePrefsPaneKeyUp (evt) {
    // first check for IME compositions and ignore
    if (evt.isComposing || evt.keyCode === 229) {
        return;
    }
    if (evt.key === "Escape") {
        evt.preventDefault();
        evt.stopPropagation();
        hidePrefsPane();
    }
}

function showPrefsPane(){
    const prefsDiv = document.getElementById('prefs');
    prefsDiv.style.display='block';
    prefsDiv.addEventListener('keyup', handlePrefsPaneKeyUp);
    document.getElementById('use_colors').focus();
}

function hidePrefsPane(){
    const prefsDiv = document.getElementById('prefs');
    prefsDiv.style.display='none';
    prefsDiv.removeEventListener('keyup', handlePrefsPaneKeyUp);
    document.getElementById('prefs_link').focus();
}

var overlay, $D, $;

function handleSmallMatchesPrefKeyUp (evt) {
    // first check for IME compositions and ignore
    if (evt.isComposing || evt.keyCode === 229) {
        return;
    }
    if (evt.key === "Escape") {
        evt.preventDefault();
        evt.stopPropagation();
        hideSmallMatchExclusions();
    }
}

function showSmallMatchExclusions(left) {
    $D = YAHOO.util.Dom;
    $E = YAHOO.util.Event;
    $ = $D.get;

    $D.setStyle('small_matches_prefs', 'top', $D.getDocumentScrollTop() + 187 + 'px');
    $D.setStyle('small_matches_prefs', 'left', left + 'px');

    $D.setStyle('small_matches_prefs', 'display', 'block');
    $D.setStyle('small_matches_prefs', 'visibility', 'visible');

        // focus the field
    $D.hasClass('exclude_by_percent_row', 'selected') ? $('exclude_by_percent_value').focus() : $('exclude_by_words_value').focus();
    $E.on(window, 'scroll', repositionDialog);



/*
A problem occurs: when the focus moves from the wordcount field to the percentage field
the existing percentage is floored. So even if the value is "correct", it gets incorrect,
because the floored percentage is different from the word count. Especially in bigger texts and smaller matches
this becomes an issue. So, the percentage displayed when updated from the word count should be the rounded one.
only when the input itself is given manually, should this override the exclusionPercent value.

This can be done as long as we use the functions below as actual keyboard handlers, so we can filter between numerical
values entered and other keys (such as tab). Moreover, we could do up and down to increase or decrease the value.

Because we will need to keep a state of the unrounded percentage, it is better to have that percentage value
closed over.

*/

    let rawPercentage = 0;
    function updateExcludePercentage(evt) {
        // prevent symbol composing to interfere, 229 is a special code for the composition key
        if (evt.isComposing || evt.keyCode === 229) {
            return;
        }
        if (evt.key === "Escape") {
            evt.preventDefault();
            evt.stopPropagation();
            hideSmallMatchExclusions();
        }
        else if (/[0-9]/.exec(evt.key) || evt.key === 'Backspace' || evt.key === 'Delete') { //only recalculate when the entered keys represent numbers
            // only when entering or changing the value in the input field, it is clear that the user
            // intended to use this specific exclusion method
            selectSmallExclusionMethod('words');
            const wordCount = this.value;

            rawPercentage = wordCount / 4594 * 100;
            $('exclude_by_percent_value').value = Math.floor(rawPercentage); // only display floored value
        }
    }

    function updateExcludeWordCount(evt) {
        // prevent symbol composing to interfere, 229 is a special code for the composition key
        if (evt.isComposing || evt.keyCode === 229) {
            return;
        }
        if (evt.key === "Escape") {
            evt.preventDefault();
            evt.stopPropagation();
            hideSmallMatchExclusions();
        }
        else if (/[0-9]/.exec(evt.key) || evt.key === 'Backspace' || evt.key === 'Delete') { //only recalculate when the entered keys represent numbers
            // only when entering or changing the value in the input field, it is clear that the user
            // intended to use this specific exclusion method
            selectSmallExclusionMethod('percent');

            const percent = this.value;
            var wordCount = Math.floor((percent/100) * 4594);

            $('exclude_by_words_value').value = wordCount;
        }
    }
    // set to global name space so hideSmallMatchesExclusions can also remove the listeners.
    if (!window.updateExcludePercentage) window.updateExcludePercentage = updateExcludePercentage;
    if (!window.updateExcludeWordCount) window.updateExcludeWordCount = updateExcludeWordCount;

    document.getElementById('small_matches_prefs').addEventListener('keyup', handleSmallMatchesPrefKeyUp);
    document.getElementById('exclude_by_words_value').addEventListener('keyup', updateExcludePercentage);
    document.getElementById('exclude_by_percent_value').addEventListener('keyup', updateExcludeWordCount);

}

function repositionDialog() {
    $D.setStyle('small_matches_prefs', 'top', $D.getDocumentScrollTop() + 187 + 'px');
}

function hideSmallMatchExclusions() {
    document.getElementById('small_matches_prefs').removeEventListener('keyup', handleSmallMatchesPrefKeyUp);
    document.getElementById('exclude_by_words_value').removeEventListener('keyup', updateExcludePercentage);
    document.getElementById('exclude_by_percent_value').removeEventListener('keyup', updateExcludeWordCount);

    $D.setStyle('small_matches_prefs', 'display', 'none');
    $E.removeListener(window, 'scroll', repositionDialog);
    document.getElementById('exclude_small_matches_link').focus();
}

function selectSmallExclusionMethod(enableType) {
    if(enableType == 'words') {
        $('exclude_by_words_value').focus();
        
        $D.addClass('exclude_by_words_row', 'selected');
        $D.removeClass('exclude_by_percent_row', 'selected');
        $D.removeClass('exclude_by_words_row', 'disabled');
        $D.addClass('exclude_by_percent_row', 'disabled');
    }
    else {
        $('exclude_by_percent_value').focus();
        
        $D.removeClass('exclude_by_words_row', 'selected');
        $D.addClass('exclude_by_percent_row', 'selected');
        $D.addClass('exclude_by_words_row', 'disabled');
        $D.removeClass('exclude_by_percent_row', 'disabled');
    }
}

function submitSmallMatchesChange() {
    var excludeBy = $D.hasClass('exclude_by_percent_row', 'selected') ? 'percent' : 'words';
    var excludeValue = excludeBy == 'percent' ? $('exclude_by_percent_value').value : $('exclude_by_words_value').value;
    
    changeSmallMatchExclusion(excludeBy, parseInt(excludeValue), 4594);
}


</script>
<input type="hidden" name="changed" value="0">
        <div class="pref_rows">
                <label for="use_colors">颜色代码匹配：</label>
                <select id="use_colors" name="use_colors" onchange="document.prefs_form.changed.value=1">
                    <option value="1">是
                    <option value="0">否 
                </select>
                <div class="clear"></div>
        </div>
        <div class="pref_rows">
                <label for="def_report_mode">预设模式：</label>
                <select id="def_report_mode" name="def_report_mode" onchange="document.prefs_form.changed.value=1">
                    <option value="0">显示所有匹配度最高的
                    <option value="1">每次显示一个匹配的
                    <option value="2">快速查看报告（经典页面）
                </select>
                <div class="clear"></div>
        </div>
        <div class="pref_rows">
                <label for="report_scrolling">自动导航</label>
                <select id="report_scrolling" name="report_scrolling" onchange="document.prefs_form.changed.value=1">
                    <option value="0">跳到下一个符合处
                    <option value="1">移动到下一个谋和处
                </select>
                <div class="clear"></div>
        </div>
        
        <div id="prefs_confirm">
            <button onClick="savePrefs()" >储存</button>  
            <button onClick="hidePrefsPane();">取消</button>
        </div>
</form>
</div>
</div>
</div>
<!-- ########################### END Preferences pop-up  ##########################--> 

<!-- ########################### BEGIN small matches pop-up  ##########################--> 
<div id="small_matches_prefs" role="dialog" aria-labelledby="exclude_small_matches_link" aria-describedby="exclude_small_matches_link" aria-owns="exclude_small_matches_link">
    <form onsubmit="submitSmallMatchesChange(); return false;">
        <ul>
            <li id="exclude_by_words_row" class="selected">
                <label for="exclude_by_words_value">字数: </label>
                <input type="text" id="exclude_by_words_value" size="3" value="" onkeyup="updateExcludePercentage"> 字
            </li>
            <li id="exclude_by_percent_row" class="disabled">
                <label for="exclude_by_percent_value">百分比: </label>
                <input type="text" id="exclude_by_percent_value" size="3" value="" max-length="3" onkeyup="updateExcludeWordCount"> %
            </li>
        </ul>
        <p><input type='submit' value="提交"> 或 <button onclick="hideSmallMatchExclusions()">取消</button></p>
    </form>
</div>

<!-- ########################### END small matches pop-up  ##########################-->

<!-- ########################### Top of Report  ##########################--> 
<div id="top">
    <div id="content" role="banner">
    
        <!-- ######### Top Bar  ##########################--> 
        <div id="top_bar">
                <ul id="top_bar_list1">
                      <!-- Preferences --><li><button id="prefs_link" onclick="showPrefsPane(); aria-haspopup="dialog">preferences</button></li>
                </ul>
                <ul id="top_bar_list2">
                      
                </ul>
                <div class="clear"></div>
        </div>   
        <!-- ######### END Top Bar  ##########################--> 
        
        
        <!-- ######### Top Body  ##########################--> 
        <div id="top_body">
        
            <div id="print_wrapper">
                <div class="general_info" role="region" aria-label="文稿资讯">
                    <!-- Logo --> 
                    <h1>
                        <span class=""></span>
                        <strong>TurnitinUK</strong>
                        <em>原创性报告</em>
                     </h1>
                 
                     <!-- Paper Info -->               
                     <ul>
                         <li>已处理到: 2024年10月05日  6:01 下午 BST</li>
                         <li>代码: 240146270 </li>
                         <li>字数: 4594</li>
                         <li>已提交: 1</li>
                     </ul>
                </div>

                 <!-- Paper Title --> 
                <h2>
                    <strong>MaxPix: detecting GAN-generated images by emphasizing local maxima_1</strong> 
                    
                    <em>整合者 60 60</em>
                    
                </h2>
            </div>
            
            <div id="similarity_print_wrapper">
                <div class="similarity_box" role="region" aria-labelledby="similarity_index_title" aria-describedby="similarity_index_title">
                    <div class="overall_similarity">
                        <div class="color_box green">&nbsp;</div>
                        <div id="similarity_index_title" class="similarity_title">相似度指标</div>
                        <div class="similarity_percent">12%</div>
                    </div>
                    <div class="similarity_by_source" role="region" aria-labelledby="similarity_by_source_title" aria-describedby="similarity_by_source_title">
                        <div id="similarity_by_source_title" class="similarity_title">依來源标示相似度</div>
                        <dl>
                            <dt>Internet&nbsp;Sources:</dt>
                            <dd>6%</dd>
                            <div class="clear"></div>
                            <dt>出版物:</dt>
                            <dd>10%</dd>
                            <div class="clear"></div>
                            <dt>学生文稿:</dt>
                            <dd>N/A</dd>
                            <div class="clear"></div>
                        </dl>
                    </div>
                </div>
            </div>
            <div class="clear"></div>
                                 
        </div>
        <!-- ######### END Top Body  ##########################--> 
        
        
    </div>
</div>
<!-- ########################### END Top of Report  ##########################--> 



<!-- ########################### TOOLBAR  ##########################--> 
<div id="index">
	<div id="toolbar_wrapper" role="toolbar">
        
	</div>
</div>
<!-- ########################### END TOOLBAR  ##########################--> 


<div class="links" role="region" aria-label="相符总览">
    <div role="list">
	<div role="listitem" aria-setsize="46" aria-posinset="1">
	    <p>
	        1% match (从 2024年09月18日 的网络)
	    </p>
        
        
 
	    <p><a href="https://ebin.pub/the-international-conference-on-image-vision-and-intelligent-systems-icivis-2021-lecture-notes-in-electrical-engineering-813-9811669627-9789811669620.html" target="_blank" style="color:#D10A0A">https://ebin.pub/the-international-conference-on-image-vision-and-intelligent-systems-icivis-2021-lecture-notes-in-electrical-engineering-813-9811669627-9789811669620.html</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="2">
	    <p>
	        1% match (从 2024年09月02日 的网络)
	    </p>
        
        
 
	    <p><a href="https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html" target="_blank" style="color:#287B28">https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="3">
	    <p>
	        1% match ("Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(816498725,37,'0')" target="_blank" style="color:blue">"Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="4">
	    <p>
	        1% match (从 2022年02月28日 的网络)
	    </p>
        
        
 
	    <p><a href="https://core.ac.uk/download/227103313.pdf" target="_blank" style="color:brown">https://core.ac.uk/download/227103313.pdf</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="5">
	    <p>
	        1% match (Haodong Li, Han Chen, Bin Li, Shunquan Tan. "Can Forensic Detectors Identify GAN Generated Images?", 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2018)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(663451619,37,'0')" target="_blank" style="color:#B64B01">Haodong Li, Han Chen, Bin Li, Shunquan Tan. "Can Forensic Detectors Identify GAN Generated Images?", 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2018</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="6">
	    <p>
	        < 1% match (从 2021年05月05日 的网络)
	    </p>
        
        
 
	    <p><a href="https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-iv-1st-ed-9783030585471-9783030585488.html" target="_blank" style="color:#630000">https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-iv-1st-ed-9783030585471-9783030585488.html</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="7">
	    <p>
	        < 1% match (从 2022年01月06日 的网络)
	    </p>
        
        
 
	    <p><a href="https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-vii-1st-ed-9783030585709-9783030585716.html" target="_blank" style="color:#0270B6">https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-vii-1st-ed-9783030585709-9783030585716.html</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="8">
	    <p>
	        < 1% match (从 2022年08月24日 的网络)
	    </p>
        
        
 
	    <p><a href="https://dokumen.pub/artificial-neural-networks-and-machine-learning-icann-2019-image-processing-28th-international-conference-on-artificial-neural-networks-munich-germany-september-1719-2019-proceedings-part-iii-1st-ed-2019-978-3-030-30507-9-978-3-030-30508-6.html" target="_blank" style="color:#330099">https://dokumen.pub/artificial-neural-networks-and-machine-learning-icann-2019-image-processing-28th-international-conference-on-artificial-neural-networks-munich-germany-september-1719-2019-proceedings-part-iii-1st-ed-2019-978-3-030-30507-9-978-3-030-30508-6.html</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="9">
	    <p>
	        < 1% match (Caie Xu, Dandan Ni, Bingyan Wang, Mingyang Wu, Honghua Gan. "Two-stage anomaly detection for positive samples and small samples based on generative adversarial networks", Multimedia Tools and Applications, 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(794456252,37,'0')" target="_blank" style="color:#227967">Caie Xu, Dandan Ni, Bingyan Wang, Mingyang Wu, Honghua Gan. "Two-stage anomaly detection for positive samples and small samples based on generative adversarial networks", Multimedia Tools and Applications, 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="10">
	    <p>
	        < 1% match ()
	    </p>
        
        
 
	    <p><a href="http://arxiv.org/abs/2212.13466" target="_blank" style="color:#CB0099">Wang, Huaming, Fei, Jianwei, Dai, Yunshu, Leng, Lingyun, Xia, Zhihua. "General GAN-generated image detection by data augmentation in  fingerprint domain", 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="11">
	    <p>
	        < 1% match (从 2024年01月04日 的网络)
	    </p>
        
        
 
	    <p><a href="https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0290303" target="_blank" style="color:#006331">https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0290303</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="12">
	    <p>
	        < 1% match ("Pattern Recognition. ICPR International Workshops and Challenges", Springer Science and Business Media LLC, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(682082615,37,'0')" target="_blank" style="color:#795AB9">"Pattern Recognition. ICPR International Workshops and Challenges", Springer Science and Business Media LLC, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="13">
	    <p>
	        < 1% match (从 2011年09月22日 的网络)
	    </p>
        
        
 
	    <p><a href="http://scalp.gforge.inria.fr/2007_09_18_kickoff/tatouage.pdf" target="_blank" style="color:#935F32">http://scalp.gforge.inria.fr/2007_09_18_kickoff/tatouage.pdf</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="14">
	    <p>
	        < 1% match (Tailong Qin, Hang Cheng, Fafa Chen. "Research on Multi-Sensor Information Fusion Technique for Motor Fault Diagnosis", 2009 2nd International Congress on Image and Signal Processing, 2009)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(584140812,37,'0')" target="_blank" style="color:#ce0031">Tailong Qin, Hang Cheng, Fafa Chen. "Research on Multi-Sensor Information Fusion Technique for Motor Fault Diagnosis", 2009 2nd International Congress on Image and Signal Processing, 2009</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="15">
	    <p>
	        < 1% match (Xu Wei, Ding Manna, Wang Weihang. "Text Detection Design Based on Deep Neural Network", Proceedings of the 2020 International Conference on Aviation Safety and Information Technology, 2020)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(675946916,37,'0')" target="_blank" style="color:#866712">Xu Wei, Ding Manna, Wang Weihang. "Text Detection Design Based on Deep Neural Network", Proceedings of the 2020 International Conference on Aviation Safety and Information Technology, 2020</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="16">
	    <p>
	        < 1% match (从 2022年05月17日 的网络)
	    </p>
        
        
 
	    <p><a href="https://koreascience.or.kr/article/JAKO201307954514061.page" target="_blank" style="color:#63009c">https://koreascience.or.kr/article/JAKO201307954514061.page</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="17">
	    <p>
	        < 1% match (Changtao Miao, Zichang Tan, Qi Chu, Huan Liu, Honggang Hu, Nenghai Yu. " F  Trans: High-Frequency Fine-Grained Transformer for Face Forgery Detection ", IEEE Transactions on Information Forensics and Security, 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(785160539,37,'0')" target="_blank" style="color:#A85503">Changtao Miao, Zichang Tan, Qi Chu, Huan Liu, Honggang Hu, Nenghai Yu. " F  Trans: High-Frequency Fine-Grained Transformer for Face Forgery Detection ", IEEE Transactions on Information Forensics and Security, 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="18">
	    <p>
	        < 1% match (Sangyup Lee, Shahroz Tariq, Youjin Shin, Simon S. Woo. "Detecting handcrafted facial image manipulations and GAN-generated facial images using Shallow-FakeFaceNet", Applied Soft Computing, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(713730171,37,'0')" target="_blank" style="color:#cc0066">Sangyup Lee, Shahroz Tariq, Youjin Shin, Simon S. Woo. "Detecting handcrafted facial image manipulations and GAN-generated facial images using Shallow-FakeFaceNet", Applied Soft Computing, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="19">
	    <p>
	        < 1% match (出版物)
	    </p>
        
        
 
	    <p><a href="https://doi.org/10.1201/9781420013610" target="_blank" style="color:#21785B">Yan Zhang, Honglin Hu, Masayuki Fujise. "Resource, Mobility, and Security Management in Wireless Networks and Mobile Communications", Auerbach Publications, 2019</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="20">
	    <p>
	        < 1% match (从 2018年03月20日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.scribd.com/document/36462071/Supplementation-of-Nitrogen-Sources-and-Growth-Factors-in-Pineapple-Waste-Extract-Medium-for-Optimum-Yeast-Candida-Utilis-Biomass-Production-1" target="_blank" style="color:#336699">https://www.scribd.com/document/36462071/Supplementation-of-Nitrogen-Sources-and-Growth-Factors-in-Pineapple-Waste-Extract-Medium-for-Optimum-Yeast-Candida-Utilis-Biomass-Production-1</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="21">
	    <p>
	        < 1% match ("ROBOT 2017: Third Iberian Robotics Conference", Springer Science and Business Media LLC, 2018)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(584359407,37,'0')" target="_blank" style="color:#D10A0A">"ROBOT 2017: Third Iberian Robotics Conference", Springer Science and Business Media LLC, 2018</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="22">
	    <p>
	        < 1% match (Julia Grabinski, Janis Keuper, Margret Keuper. "Aliasing and adversarial robust generalization of CNNs", Machine Learning, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(780991561,37,'0')" target="_blank" style="color:#287B28">Julia Grabinski, Janis Keuper, Margret Keuper. "Aliasing and adversarial robust generalization of CNNs", Machine Learning, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="23">
	    <p>
	        < 1% match (Lin Cao, Wenjun Sheng, Fan Zhang, Kangning Du, Chong Fu, Peiran Song. "Face Manipulation Detection Based on Supervised Multi-Feature Fusion Attention Network", Sensors, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(749428211,37,'0')" target="_blank" style="color:blue">Lin Cao, Wenjun Sheng, Fan Zhang, Kangning Du, Chong Fu, Peiran Song. "Face Manipulation Detection Based on Supervised Multi-Feature Fusion Attention Network", Sensors, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="24">
	    <p>
	        < 1% match (Miaomiao Yu, Jun Zhang, Shuohao Li, Jun Lei. "MSFRNet: Two‐stream deep forgery detector via multi‐scale feature extraction", IET Image Processing, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(786899956,37,'0')" target="_blank" style="color:brown">Miaomiao Yu, Jun Zhang, Shuohao Li, Jun Lei. "MSFRNet: Two‐stream deep forgery detector via multi‐scale feature extraction", IET Image Processing, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="25">
	    <p>
	        < 1% match (Riccardo Corvi, Davide Cozzolino, Giovanni Poggi, Koki Nagano, Luisa Verdoliva. "Intriguing properties of synthetic images: 从 generative adversarial networks to diffusion models", 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(801657847,37,'0')" target="_blank" style="color:#B64B01">Riccardo Corvi, Davide Cozzolino, Giovanni Poggi, Koki Nagano, Luisa Verdoliva. "Intriguing properties of synthetic images: from generative adversarial networks to diffusion models", 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="26">
	    <p>
	        < 1% match (从 2024年08月02日 的网络)
	    </p>
        
        
 
	    <p><a href="https://tdr.lib.ntu.edu.tw/jspui/bitstream/123456789/88160/1/ntu-111-2.pdf" target="_blank" style="color:#630000">https://tdr.lib.ntu.edu.tw/jspui/bitstream/123456789/88160/1/ntu-111-2.pdf</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="27">
	    <p>
	        < 1% match (从 2023年10月09日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.jsjkx.com/CN/Y2023/V50/I6/216" target="_blank" style="color:#0270B6">https://www.jsjkx.com/CN/Y2023/V50/I6/216</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="28">
	    <p>
	        < 1% match ("Integration of Constraint Programming, Artificial Intelligence, and Operations Research", Springer Science and Business Media LLC, 2019)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(637806348,37,'0')" target="_blank" style="color:#330099">"Integration of Constraint Programming, Artificial Intelligence, and Operations Research", Springer Science and Business Media LLC, 2019</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="29">
	    <p>
	        < 1% match (Binxu Wang, Carlos R. Ponce. "Neural Dynamics of Object Manifold Alignment in the Ventral Stream", Cold Spring Harbor Laboratory, 2024)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(821757421,3160,'0')" target="_blank" style="color:#227967">Binxu Wang, Carlos R. Ponce. "Neural Dynamics of Object Manifold Alignment in the Ventral Stream", Cold Spring Harbor Laboratory, 2024</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="30">
	    <p>
	        < 1% match (Chengdong Dong, Ajay Kumar, Eryun Liu. "Think Twice Before Detecting GAN-generated Fake Images 从 their Spectral Domain Imprints", 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(776316938,37,'0')" target="_blank" style="color:#CB0099">Chengdong Dong, Ajay Kumar, Eryun Liu. "Think Twice Before Detecting GAN-generated Fake Images from their Spectral Domain Imprints", 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="31">
	    <p>
	        < 1% match (Kai Zeng, Xiangyu Yu, Beibei Liu, Yu Guan, Yongjian Hu. "Detecting Deepfakes in Alternative Color Spaces to Withstand Unseen Corruptions", 2023 11th International Workshop on Biometrics and Forensics (IWBF), 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(799471356,37,'0')" target="_blank" style="color:#006331">Kai Zeng, Xiangyu Yu, Beibei Liu, Yu Guan, Yongjian Hu. "Detecting Deepfakes in Alternative Color Spaces to Withstand Unseen Corruptions", 2023 11th International Workshop on Biometrics and Forensics (IWBF), 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="32">
	    <p>
	        < 1% match (Qiang Xu, Zhe Wang, Zhongjie Mi, Hong Yan. "Exposing Computer-Generated Images Via Amplified Texture Differences Learning", 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(814146874,37,'0')" target="_blank" style="color:#795AB9">Qiang Xu, Zhe Wang, Zhongjie Mi, Hong Yan. "Exposing Computer-Generated Images Via Amplified Texture Differences Learning", 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="33">
	    <p>
	        < 1% match (Zheling Meng, Bo Peng, Jing Dong, Tieniu Tan, Haonan Cheng. "Artifact feature purification for cross-domain detection of AI-generated images", Computer Vision and Image Understanding, 2024)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(824372936,37,'0')" target="_blank" style="color:#935F32">Zheling Meng, Bo Peng, Jing Dong, Tieniu Tan, Haonan Cheng. "Artifact feature purification for cross-domain detection of AI-generated images", Computer Vision and Image Understanding, 2024</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="34">
	    <p>
	        < 1% match (Ziyuan Cheng, Yiyang Wang, Yongjing Wan, Cuiling Jiang. "DeepFake detection method based on multi-scale interactive dual-stream network", Journal of Visual Communication and Image Representation, 2024)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(825380872,37,'0')" target="_blank" style="color:#ce0031">Ziyuan Cheng, Yiyang Wang, Yongjing Wan, Cuiling Jiang. "DeepFake detection method based on multi-scale interactive dual-stream network", Journal of Visual Communication and Image Representation, 2024</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="35">
	    <p>
	        < 1% match (从 2024年02月16日 的网络)
	    </p>
        
        
 
	    <p><a href="https://export.arxiv.org/pdf/2310.07552" target="_blank" style="color:#866712">https://export.arxiv.org/pdf/2310.07552</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="36">
	    <p>
	        < 1% match (从 2022年09月23日 的网络)
	    </p>
        
        
 
	    <p><a href="https://learn.microsoft.com/zh-tw/windows/ai/windows-ml/tutorials/pytorch-train-model" target="_blank" style="color:#63009c">https://learn.microsoft.com/zh-tw/windows/ai/windows-ml/tutorials/pytorch-train-model</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="37">
	    <p>
	        < 1% match (从 2023年08月12日 的网络)
	    </p>
        
        
 
	    <p><a href="https://mdpi-res.com/d_attachment/electronics/electronics-12-03407/article_deploy/electronics-12-03407.pdf?version=1691681115" target="_blank" style="color:#A85503">https://mdpi-res.com/d_attachment/electronics/electronics-12-03407/article_deploy/electronics-12-03407.pdf?version=1691681115</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="38">
	    <p>
	        < 1% match (从 2024年05月11日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.coursehero.com/file/39105282/2-Joshua-Rothman-Why-Is-Academic-Writing-So-Academicpdf/" target="_blank" style="color:#cc0066">https://WWW.coursehero.com/file/39105282/2-Joshua-Rothman-Why-Is-Academic-Writing-So-Academicpdf/</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="39">
	    <p>
	        < 1% match (从 2024年07月13日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.springerprofessional.de/En/detection-of-ai-generated-synthetic-faces/20085052" target="_blank" style="color:#21785B">https://www.springerprofessional.de/En/detection-of-ai-generated-synthetic-faces/20085052</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="40">
	    <p>
	        < 1% match ("Computer Security – ESORICS 2024", Springer Science and Business Media LLC, 2024)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(825373072,37,'0')" target="_blank" style="color:#336699">"Computer Security – ESORICS 2024", Springer Science and Business Media LLC, 2024</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="41">
	    <p>
	        < 1% match ("Simulation and Synthesis in Medical Imaging", Springer Science and Business Media LLC, 2017)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(584247233,37,'0')" target="_blank" style="color:#D10A0A">"Simulation and Synthesis in Medical Imaging", Springer Science and Business Media LLC, 2017</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="42">
	    <p>
	        < 1% match (Haifeng Li, Jianping Zong, Jingjing Nie, Zhilong Wu, Hongyang Han. "Pavement crack detection algorithm based on densely connected and deeply supervised network", IEEE Access, 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(677710540,37,'0')" target="_blank" style="color:#287B28">Haifeng Li, Jianping Zong, Jingjing Nie, Zhilong Wu, Hongyang Han. "Pavement crack detection algorithm based on densely connected and deeply supervised network", IEEE Access, 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="43">
	    <p>
	        < 1% match (Mingxu Zhang, Hongxia Wang, Peisong He, Asad Malik, Hanqing Liu. "Exposing unseen GAN-generated image using unsupervised domain adaptation", Knowledge-Based Systems, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(818808010,37,'0')" target="_blank" style="color:blue">Mingxu Zhang, Hongxia Wang, Peisong He, Asad Malik, Hanqing Liu. "Exposing unseen GAN-generated image using unsupervised domain adaptation", Knowledge-Based Systems, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="44">
	    <p>
	        < 1% match (Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, Houqiang Li. "DIRE for Diffusion-Generated Image Detection", 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(813495405,37,'0')" target="_blank" style="color:brown">Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, Houqiang Li. "DIRE for Diffusion-Generated Image Detection", 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="45">
	    <p>
	        < 1% match ("Computer Vision – ECCV 2018", Springer Science and Business Media LLC, 2018)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(616810748,37,'0')" target="_blank" style="color:#B64B01">"Computer Vision – ECCV 2018", Springer Science and Business Media LLC, 2018</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="46" aria-posinset="46">
	    <p>
	        < 1% match (Lecture Notes in Computer Science, 2007.)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(55432999,37,'0')" target="_blank" style="color:#630000">Lecture Notes in Computer Science, 2007.</a>

    
    </p>
    </div></div></div></div><div id="body" tabIndex="0" role="main"><p>Contributions Computer Applications Research Revision Date:2024/10/05 MaxPix: detecting GAN-generated images by emphasizing local maxima Ronghao Dai1, 2a, Lingxi Peng1, 2b† (1. Guangzhou University, School <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: https://tdr.lib.ntu.edu.tw/jspui/bitstream/123456789/88160/1/ntu-111-2.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=1676247118&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">of Computer Science and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Network <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: https://tdr.lib.ntu.edu.tw/jspui/bitstream/123456789/88160/1/ntu-111-2.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=1676247118&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">Engineering</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">; 2. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: https://tdr.lib.ntu.edu.tw/jspui/bitstream/123456789/88160/1/ntu-111-2.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=1676247118&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">College of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Mechanical and <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: https://tdr.lib.ntu.edu.tw/jspui/bitstream/123456789/88160/1/ntu-111-2.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=1676247118&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">Electrical Engineering</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">; Guangzhou 510000, Guangdong, China) Abstract: The realistic images generated by GANs(Generative Adversarial Networks) enrich people's lives, but they also pose serious threats to personal privacy and society, and it has become essential to study algorithms that <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: Haodong Li, Han Chen, Bin Li, Shunquan Tan. "Can Forensic Detectors Identify GAN Generated Images?", 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.23919/APSIPA.2018.8659461', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">can accurately detect GAN-generated images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. Existing studies use artifacts <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 30 in source list: Chengdong Dong, Ajay Kumar, Eryun Liu. "Think Twice Before Detecting GAN-generated Fake Images from their Spectral Domain Imprints", 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/CVPR52688.2022.00771', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">to detect GAN-generated images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, but <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 30 in source list: Chengdong Dong, Ajay Kumar, Eryun Liu. "Think Twice Before Detecting GAN-generated Fake Images from their Spectral Domain Imprints", 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/CVPR52688.2022.00771', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">the artifacts</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> present <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 30 in source list: Chengdong Dong, Ajay Kumar, Eryun Liu. "Think Twice Before Detecting GAN-generated Fake Images from their Spectral Domain Imprints", 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/CVPR52688.2022.00771', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">in</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> different GAN-generated images vary widely, and thus the cross-model generalization performance of such algorithms is weak. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=3065638588&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">In this</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> thesis, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=3065638588&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">we propose</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the MaxPix, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=3065638588&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">a</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> new <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=3065638588&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">algorithm based on</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the combination of statistical features and deep learning techniques, for generating image detection. Firstly, MaxPix obtains the filter map of the image by designing the MaxSel filtering algorithm and then designs MA Block embedded in ResNet (Residual Network) to obtain MResNet. MaxPix finally utilizes MResNet to extract features from the filter map to detect GAN-generated images. Experimental results on publicly available datasets such as Wang and Faces-HQ show that the detection accuracy of MaxPix reaches 85.9% and 99.6% on average, which improves 7.6% and 10.2% relative <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 28 in source list: "Integration of Constraint Programming, Artificial Intelligence, and Operations Research", Springer Science and Business Media LLC, 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-19212-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">to state-of-the-art algorithms</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> such as <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 28 in source list: "Integration of Constraint Programming, Artificial Intelligence, and Operations Research", Springer Science and Business Media LLC, 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-19212-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> NAFID and <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 28 in source list: "Integration of Constraint Programming, Artificial Intelligence, and Operations Research", Springer Science and Business Media LLC, 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-19212-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> GocNet. Thus MaxPix has strong cross-model generalization performance. Keywords: gan; generative images; artifacts; cross-model generalization 0 Introduction Digital images have become one of the main carriers for transmitting network information due to the advantages of diverse contents and convenient storage, and have been widely used in the fields of news, information, medical diagnosis, and identification, etc. GANs[1] (Generative Adversarial Networks) is a generative model based on deep learning technology, which was <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://ebin.pub/the-international-conference-on-image-vision-and-intelligent-systems-icivis-2021-lecture-notes-in-electrical-engineering-813-9811669627-9789811669620.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=44965040&n=3810&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">proposed by</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Ian <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://ebin.pub/the-international-conference-on-image-vision-and-intelligent-systems-icivis-2021-lecture-notes-in-electrical-engineering-813-9811669627-9789811669620.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=44965040&n=3810&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">Goodfellow et al</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. GAN <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://ebin.pub/the-international-conference-on-image-vision-and-intelligent-systems-icivis-2021-lecture-notes-in-electrical-engineering-813-9811669627-9789811669620.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=44965040&n=3810&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">consists of a generator and a discriminator, in</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> which <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: https://dokumen.pub/artificial-neural-networks-and-machine-learning-icann-2019-image-processing-28th-international-conference-on-artificial-neural-networks-munich-germany-september-1719-2019-proceedings-part-iii-1st-ed-2019-978-3-030-30507-9-978-3-030-30508-6.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=612376102&n=3799&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">the generator can generate samples similar to real</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> data. Until now, more than a hundred different GANs capable of generating images have been produced, and these generated images have enriched people's lives. However, some people maliciously use GANs to forge images and abuse them in politics and pornography, posing a serious threat to personal privacy and society. Since digital images are widely used in various fields, their authenticity is very important. In order Tto prevent GAN-generated images from being abused and bringing harm to the society, effective ---------- detection algorithms are needed to detect whether images are generated by GAN or not, so as to help people correctly distinguish real images from generated images. Currently, researchers have proposed a large number of detection algorithms to detect GAN-generated images. These algorithms are mainly categorized into <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 42 in source list: Haifeng Li, Jianping Zong, Jingjing Nie, Zhilong Wu, Hongyang Han. "Pavement crack detection algorithm based on densely connected and deeply supervised network", IEEE Access, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ACCESS.2021.3050401', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">detection algorithms based on traditional digital image</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> forensic <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: Xu Wei, Ding Manna, Wang Weihang. "Text Detection Design Based on Deep Neural Network", Proceedings of the 2020 International Conference on Aviation Safety and Information Technology, 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1145/3434581.3434705', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">methods and detection algorithms based on deep learning</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> techniques. In <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: Xu Wei, Ding Manna, Wang Weihang. "Text Detection Design Based on Deep Neural Network", Proceedings of the 2020 International Conference on Aviation Safety and Information Technology, 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1145/3434581.3434705', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">the detection</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> algorithms <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: Xu Wei, Ding Manna, Wang Weihang. "Text Detection Design Based on Deep Neural Network", Proceedings of the 2020 International Conference on Aviation Safety and Information Technology, 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1145/3434581.3434705', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">based on traditional</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> digital image forensics, researchers mainly design detection algorithms <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 39 in source list: https://www.springerprofessional.de/En/detection-of-ai-generated-synthetic-faces/20085052"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=391145612&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">to detect generated images based on the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> properties <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 39 in source list: https://www.springerprofessional.de/En/detection-of-ai-generated-synthetic-faces/20085052"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=391145612&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> digital images such as illumination inconsistency and, statistical properties in <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 34 in source list: Ziyuan Cheng, Yiyang Wang, Yongjing Wan, Cuiling Jiang. "DeepFake detection method based on multi-scale interactive dual-stream network", Journal of Visual Communication and Image Representation, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.jvcir.2024.104263', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">the spatial domain and frequency domain</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. McCloskey[3]. analyzed <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 34 in source list: Ziyuan Cheng, Yiyang Wang, Yongjing Wan, Cuiling Jiang. "DeepFake detection method based on multi-scale interactive dual-stream network", Journal of Visual Communication and Image Representation, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.jvcir.2024.104263', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> process <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 34 in source list: Ziyuan Cheng, Yiyang Wang, Yongjing Wan, Cuiling Jiang. "DeepFake detection method based on multi-scale interactive dual-stream network", Journal of Visual Communication and Image Representation, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.jvcir.2024.104263', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> color formation in images and argued that the normalization process present in GAN restricts the range of pixels in the generated image, making the exposure of the generated image different from that of a real image differently. They proposed to use the measured frequency of overexposure and underexposure of the image as a feature to Computer Applications Research Contributions detect the generated image. However, the algorithm only achieves an AUC (Area Under Curve) value of 0.7. Durall[4] found that <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 32 in source list: Qiang Xu, Zhe Wang, Zhongjie Mi, Hong Yan. "Exposing Computer-Generated Images Via Amplified Texture Differences Learning", 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/SMC53992.2023.10394157', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">the high-frequency components of the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> generated <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 32 in source list: Qiang Xu, Zhe Wang, Zhongjie Mi, Hong Yan. "Exposing Computer-Generated Images Via Amplified Texture Differences Learning", 2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/SMC53992.2023.10394157', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">images are</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> distorted and proposed <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 11 in source list: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0290303"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=1040493641&n=3806&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">to use the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> azimuthal integral <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 11 in source list: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0290303"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=1040493641&n=3806&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">of the image as a feature</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> to <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 11 in source list: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0290303"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=1040493641&n=3806&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">detect</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the generated images through the support vector machine, which achieves 100% accuracy. However, the algorithm lacks cross-model generalization performance. Guo[5] believed that the eye pupils in real face images are elliptical, while the eye pupils in generated face images are irregular. They proposed an algorithm to determine whether an image belongs to a generated image or not by calculating the IoU[6] values of the pupil region and the elliptical mask, which and judgied whether the image belongs to a generated image by the IoU value. This algorithm has strict requirements on the quality and angle of the image, and if there are defects in the human physiology, it will make the algorithm misjudged. Liu[7] <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 29 in source list: Binxu Wang, Carlos R. Ponce. "Neural Dynamics of Object Manifold Alignment in the Ventral Stream", Cold Spring Harbor Laboratory, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1101/2024.06.20.596072', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">used the Sobel operator to</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> get <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 29 in source list: Binxu Wang, Carlos R. Ponce. "Neural Dynamics of Object Manifold Alignment in the Ventral Stream", Cold Spring Harbor Laboratory, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1101/2024.06.20.596072', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">the gradient</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> of the image <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 29 in source list: Binxu Wang, Carlos R. Ponce. "Neural Dynamics of Object Manifold Alignment in the Ventral Stream", Cold Spring Harbor Laboratory, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1101/2024.06.20.596072', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">in</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> HSV (Hue, Saturation, Value) space and count histograms of the gradient distribution as a feature to detect the generated images. The aAlgorithm achieved 99.4% accuracy when detecting the images generated by PGGAN[8], but cross-model generalization performance was not investigated. Detection algorithms based on traditional image forensics have theoretical and experimental foundations. However, such algorithms are highly susceptible to overfitting statistical features that exist only in the trainset, while different GAN-generated images have different statistical features and thus tend to have lower accuracy in detecting unknown GAN-generated images. In addition, These algorithms require the images to conform to a specific angle and quality, which also limits the application of the algorithms. Detection algorithms based on deep learning techniques utilize neural networks to construct algorithmic models and learn general features from massive data to detect the generated images. Since neural networks have strong representational ability, these algorithms generalizezes well and attracts many scholars to study. The up-sampling process is almost common to GANs. Zhang[9] designed AutoGAN containing an up-sampling process to generate <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 6 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-iv-1st-ed-9783030585471-9783030585488.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=657331371&n=3796&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">a large number of images that</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> simulate <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 6 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-iv-1st-ed-9783030585471-9783030585488.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=657331371&n=3796&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">a</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> variety <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 6 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-iv-1st-ed-9783030585471-9783030585488.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=657331371&n=3796&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> generated images and used such images to train the algorithm. However, the detection accuracy of the algorithm will be severely degraded if the up-sampling method used by the GAN is significantly different from that used by AutoGAN. Liu[10] found that the phase spectrum of the image retains rich frequency components and proposed to combine the image <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 27 in source list: https://www.jsjkx.com/CN/Y2023/V50/I6/216"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=15381426&n=3805&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">spatial domain features</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> and <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 27 in source list: https://www.jsjkx.com/CN/Y2023/V50/I6/216"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=15381426&n=3805&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> phase <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 27 in source list: https://www.jsjkx.com/CN/Y2023/V50/I6/216"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=15381426&n=3805&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">features to detect the generated</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> images. The algorithm detects the two Deepfake datasets[11,12] obtaining an accuracy rate of 91.5% and 76.88%. Jeong[<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 46 in source list: Lecture Notes in Computer Science, 2007."><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-540-74976-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">13] proposed an algorithm that uses</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> a high-pass filter to remove irrelevant features <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 25 in source list: Riccardo Corvi, Davide Cozzolino, Giovanni Poggi, Koki Nagano, Luisa Verdoliva. "Intriguing properties of synthetic images: from generative adversarial networks to diffusion models", 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/CVPRW59228.2023.00104', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">in the spatial domain and frequency domain for</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> highlighting <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 25 in source list: Riccardo Corvi, Davide Cozzolino, Giovanni Poggi, Koki Nagano, Luisa Verdoliva. "Intriguing properties of synthetic images: from generative adversarial networks to diffusion models", 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/CVPRW59228.2023.00104', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> important features to detect the generated image, which obtains more than 72% cross-model detection accuracy and average precision. Tian[14] divided the image frequency components <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 35 in source list: https://export.arxiv.org/pdf/2310.07552"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=3540691194&n=3806&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">into low</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, medium, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 35 in source list: https://export.arxiv.org/pdf/2310.07552"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=3540691194&n=3806&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">and high components, and then</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> aggregated the features <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 35 in source list: https://export.arxiv.org/pdf/2310.07552"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=3540691194&n=3806&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">with</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the original image. They utilized the aggregated features to detect the generated image and obtains an accuracy of 97.74%. Wang[15] used <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: https://koreascience.or.kr/article/JAKO201307954514061.page"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=1769681677&n=3798&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">wavelet transform to transform the image</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> in <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: https://koreascience.or.kr/article/JAKO201307954514061.page"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=1769681677&n=3798&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">the spatial domain to the frequency domain</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, then extracted <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 31 in source list: Kai Zeng, Xiangyu Yu, Beibei Liu, Yu Guan, Yongjian Hu. "Detecting Deepfakes in Alternative Color Spaces to Withstand Unseen Corruptions", 2023 11th International Workshop on Biometrics and Forensics (IWBF), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/IWBF57495.2023.10157416', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">the high- frequency</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> components <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 31 in source list: Kai Zeng, Xiangyu Yu, Beibei Liu, Yu Guan, Yongjian Hu. "Detecting Deepfakes in Alternative Color Spaces to Withstand Unseen Corruptions", 2023 11th International Workshop on Biometrics and Forensics (IWBF), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/IWBF57495.2023.10157416', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">in the image</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> and fused <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 31 in source list: Kai Zeng, Xiangyu Yu, Beibei Liu, Yu Guan, Yongjian Hu. "Detecting Deepfakes in Alternative Color Spaces to Withstand Unseen Corruptions", 2023 11th International Workshop on Biometrics and Forensics (IWBF), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/IWBF57495.2023.10157416', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> features with <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 31 in source list: Kai Zeng, Xiangyu Yu, Beibei Liu, Yu Guan, Yongjian Hu. "Detecting Deepfakes in Alternative Color Spaces to Withstand Unseen Corruptions", 2023 11th International Workshop on Biometrics and Forensics (IWBF), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/IWBF57495.2023.10157416', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> original image. Algorithms detected the generated image by Xception[16] and achieved more than 98% accuracy, but the accuracy of detecting the low-quality image is lower. Miao[17] designed the Center Differential Attention Transformer to make the algorithm learn global <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Changtao Miao, Zichang Tan, Qi Chu, Huan Liu, Honggang Hu, Nenghai Yu. " F  Trans: High-Frequency Fine-Grained Transformer for Face Forgery Detection ", IEEE Transactions on Information Forensics and Security, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/TIFS.2022.3233774', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">high- frequency information and local fine-grained</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> features and designed <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Changtao Miao, Zichang Tan, Qi Chu, Huan Liu, Honggang Hu, Nenghai Yu. " F  Trans: High-Frequency Fine-Grained Transformer for Face Forgery Detection ", IEEE Transactions on Information Forensics and Security, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/TIFS.2022.3233774', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">a high- frequency wavelet sampler</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> to make the algorithm extract multi-channel high- frequency features. The proposed algorithm aggregated the two features to detect the generated image, but the accuracy of detecting the compressed processed image is low. Algorithms based on deep learning techniques generally need to utilize the artifacts which brought by the imperfect design of the <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: Haodong Li, Han Chen, Bin Li, Shunquan Tan. "Can Forensic Detectors Identify GAN Generated Images?", 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.23919/APSIPA.2018.8659461', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">GAN to detect the generated images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. However, with <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: Haodong Li, Han Chen, Bin Li, Shunquan Tan. "Can Forensic Detectors Identify GAN Generated Images?", 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.23919/APSIPA.2018.8659461', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> improvement <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: Haodong Li, Han Chen, Bin Li, Shunquan Tan. "Can Forensic Detectors Identify GAN Generated Images?", 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.23919/APSIPA.2018.8659461', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the GAN structure, the obvious artifacts in the generated images have been effectively hidden. In addition, the artifacts generated by different GANs vary, which limits the generalization performance of artifact-dependent detection algorithms, resulting in low accuracy when detecting unknown GAN-generated images and a lack of generality of the algorithms. In view of Given this, this thesis proposes to investigate detection algorithms that do not need to utilize artifacts <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: Haodong Li, Han Chen, Bin Li, Shunquan Tan. "Can Forensic Detectors Identify GAN Generated Images?", 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.23919/APSIPA.2018.8659461', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">to detect</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: Haodong Li, Han Chen, Bin Li, Shunquan Tan. "Can Forensic Detectors Identify GAN Generated Images?", 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.23919/APSIPA.2018.8659461', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">generated images. In this</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> thesis, the pixel value distribution of the GAN-generated images that <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 37 in source list: https://mdpi-res.com/d_attachment/electronics/electronics-12-03407/article_deploy/electronics-12-03407.pdf?version=1691681115"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=629085850&n=3804&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">generated by GAN such as StarGAN</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">[18],<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 37 in source list: https://mdpi-res.com/d_attachment/electronics/electronics-12-03407/article_deploy/electronics-12-03407.pdf?version=1691681115"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=3.5681902811568&svr=6&lang=zh_hans&sid=629085850&n=3804&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">and StyleGAN2</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">[19], and real images in the datasets such as FFHQ[20] ,and CelebA are counted., and Iit is <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 22 in source list: Julia Grabinski, Janis Keuper, Margret Keuper. "Aliasing and adversarial robust generalization of CNNs", Machine Learning, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s10994-022-06222-8', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">observed that</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 22 in source list: Julia Grabinski, Janis Keuper, Margret Keuper. "Aliasing and adversarial robust generalization of CNNs", Machine Learning, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s10994-022-06222-8', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">generated images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> cannot <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 22 in source list: Julia Grabinski, Janis Keuper, Margret Keuper. "Aliasing and adversarial robust generalization of CNNs", Machine Learning, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s10994-022-06222-8', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">reproduce the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> pixel <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 22 in source list: Julia Grabinski, Janis Keuper, Margret Keuper. "Aliasing and adversarial robust generalization of CNNs", Machine Learning, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s10994-022-06222-8', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">distribution of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the real <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 22 in source list: Julia Grabinski, Janis Keuper, Margret Keuper. "Aliasing and adversarial robust generalization of CNNs", Machine Learning, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s10994-022-06222-8', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, and there are more points with larger pixel values in the real images than in the generated images. Therefore, this thesis proposes the MaxPix detection algorithm based on statistical features. Firstly, this thesis proposes the MaxSel algorithm for performing filtering on the images, and Computer Applications Research Contributions then designs the MA Block embedded in ResNet to form MResNet, which <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 33 in source list: Zheling Meng, Bo Peng, Jing Dong, Tieniu Tan, Haonan Cheng. "Artifact feature purification for cross-domain detection of AI-generated images", Computer Vision and Image Understanding, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.cviu.2024.104078', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">is used to extract features from</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the filtered <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 33 in source list: Zheling Meng, Bo Peng, Jing Dong, Tieniu Tan, Haonan Cheng. "Artifact feature purification for cross-domain detection of AI-generated images", Computer Vision and Image Understanding, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.cviu.2024.104078', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> to detect <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 33 in source list: Zheling Meng, Bo Peng, Jing Dong, Tieniu Tan, Haonan Cheng. "Artifact feature purification for cross-domain detection of AI-generated images", Computer Vision and Image Understanding, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.cviu.2024.104078', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> generated images. Numerous experiments show <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: Haodong Li, Han Chen, Bin Li, Shunquan Tan. "Can Forensic Detectors Identify GAN Generated Images?", 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.23919/APSIPA.2018.8659461', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">the effectiveness of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> MaxPix <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: Haodong Li, Han Chen, Bin Li, Shunquan Tan. "Can Forensic Detectors Identify GAN Generated Images?", 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.23919/APSIPA.2018.8659461', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">for detecting generated images. The</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> contribution of this thesis is as follows: Based on the characteristic that GAN-generated images cannot reproduce the pixel value distribution condition of real images, the MaxPix detection algorithm is proposed <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 44 in source list: Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, Houqiang Li. "DIRE for Diffusion-Generated Image Detection", 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICCV51070.2023.02051', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">to detect</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> GAN- <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 44 in source list: Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, Houqiang Li. "DIRE for Diffusion-Generated Image Detection", 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICCV51070.2023.02051', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">generated images and the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> MaxSel is <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 44 in source list: Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, Houqiang Li. "DIRE for Diffusion-Generated Image Detection", 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICCV51070.2023.02051', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">proposed</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> for filtering images. MaxPix detects the Wang[21] dataset and the Faces-HQ[4] dataset with an average accuracy of 85.9% and 99.6%, which is an improvement of 7.6% and 10.2% compared to current <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">state-of-the-art</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> detection <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">algorithms</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. Thus <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> MaxPix has strong <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">cross</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">-model <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">generalization</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> performance. 1. Algorithm Ddescription Durall[4] found that GAN-generated images cannot reproduce the spectral distribution of real images. He[22] found that the generated images have stronger nonlocal similarity than real images, which inspired this thesis to explore whether there is any difference in the pixel distribution <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 6 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-iv-1st-ed-9783030585471-9783030585488.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=657331371&n=3796&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">between the generated and real images. For</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> this purpose, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 6 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-iv-1st-ed-9783030585471-9783030585488.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=657331371&n=3796&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> frequency of image pixel values in each pixel value range is counted and displayed using histograms in this thesis. In the experiment, the range of pixel values was divided into 60 groups. The experiments counted a total of 34k images including images generated by BigGAN[23], StarGAN and StyleGAN2,, and real images samplied from ImageNet[24], CelebA and FFHQ datasets, which are from Wang dataset[21] and Faces-HQ[4]. As shown in Fig.1, although the above- mentioned GANs are trained with a <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 19 in source list: Yan Zhang, Honglin Hu, Masayuki Fujise. "Resource, Mobility, and Security Management in Wireless Networks and Mobile Communications", Auerbach Publications, 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=823798119&n=1422&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">large number of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> real images, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 19 in source list: Yan Zhang, Honglin Hu, Masayuki Fujise. "Resource, Mobility, and Security Management in Wireless Networks and Mobile Communications", Auerbach Publications, 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=823798119&n=1422&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">it is</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> still <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 19 in source list: Yan Zhang, Honglin Hu, Masayuki Fujise. "Resource, Mobility, and Security Management in Wireless Networks and Mobile Communications", Auerbach Publications, 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=823798119&n=1422&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">difficult to</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> mimic <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 19 in source list: Yan Zhang, Honglin Hu, Masayuki Fujise. "Resource, Mobility, and Security Management in Wireless Networks and Mobile Communications", Auerbach Publications, 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=823798119&n=1422&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> distribution of pixel <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 19 in source list: Yan Zhang, Honglin Hu, Masayuki Fujise. "Resource, Mobility, and Security Management in Wireless Networks and Mobile Communications", Auerbach Publications, 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=823798119&n=1422&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">values of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> real images. Obviously, Tthere are more points in the larger pixel value range in the real image than in the generated image. Therefore, this thesis proposes the MaxPix detection algorithm, which detects the generated image by emphasizing the local maxima of the image and using the maxima features. Fig1 Statistical distribution of pixel values of BigGAN, StarGAN and StyleGAN2 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">generated images and real images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 2. Algorithmic Fframework <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">As</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> shown <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">in Fig</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">.2, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> MaxPix structure consistsing of a filtering module (or feature select module), <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=3065638588&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">a feature extraction network</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> MResNet, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=3065638588&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">and a classifier</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> C. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=3065638588&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">The</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> filtering module is a feature extraction network. The filtering module uses the MaxSel filtering algorithm proposed in this thesis to perform filtering on the image, making it easy for MResNet to learn distinguishable features to detect GAN-generated images. Feature select module MResNet C 0 ... 1 Fig2 MaxPix framework where the Feature select module does not update parameters Field Code Changed Computer Applications Research Contributions 2.1 MaxSel Filter MaxPix uses the convolution kernel as in equation (1) as a filter kernel to perform convolution operation with the image to obtain the filtered image. First, MaxPix splits the image channel-by-channel and performs the convolution operation using four convolution kernels to obtain the convolution values X(c, i, j ) (α1, α2 , α3 , α4 ) in four directions for each point of the three channels. Then, MaxSel compares the convolutional values of the 4 directions at the corresponding location within the group and takes the largest convolutional value among them as the filter value. For X(<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: Tailong Qin, Hang Cheng, Fafa Chen. "Research on Multi-Sensor Information Fusion Technique for Motor Fault Diagnosis", 2009 2nd International Congress on Image and Signal Processing, 2009"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/CISP.2009.5304182', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">c, i, j ) (α1, α2, α3, α4</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> )X(<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: Tailong Qin, Hang Cheng, Fafa Chen. "Research on Multi-Sensor Information Fusion Technique for Motor Fault Diagnosis", 2009 2nd International Congress on Image and Signal Processing, 2009"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/CISP.2009.5304182', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">c,i,j)(α1, α2 ,α3 ,α4</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> ), the maximum value is selected from α1, α2 ,α3 and α4 . ?<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 13 in source list: http://scalp.gforge.inria.fr/2007_09_18_kickoff/tatouage.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=1837300397&n=943&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">0 1 0</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">? ?0 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 13 in source list: http://scalp.gforge.inria.fr/2007_09_18_kickoff/tatouage.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=1837300397&n=943&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">k1 ? ?0 ? ?1</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> ??0 ? 2 00???, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 13 in source list: http://scalp.gforge.inria.fr/2007_09_18_kickoff/tatouage.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=1837300397&n=943&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">k2 1 ??0 ?1 0 0</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">? ?0 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 13 in source list: http://scalp.gforge.inria.fr/2007_09_18_kickoff/tatouage.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=1837300397&n=943&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">k3</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> ? ???00 ?2 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 13 in source list: http://scalp.gforge.inria.fr/2007_09_18_kickoff/tatouage.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=1837300397&n=943&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">0? , k4 ? ?0</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 0 1?? ??1 0 ?2 0 0? 1? 0?? 0 ?2 0 1? 0? 0?? (1) As shown in Equation(2), where X(c, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 45 in source list: "Computer Vision – ECCV 2018", Springer Science and Business Media LLC, 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-01249-6', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">i, j</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> )X(<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 45 in source list: "Computer Vision – ECCV 2018", Springer Science and Business Media LLC, 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-01249-6', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">c,i,j</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">) denotes <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 45 in source list: "Computer Vision – ECCV 2018", Springer Science and Business Media LLC, 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-01249-6', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> filter <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=3065638588&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">value at</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=3065638588&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">position (i, j) of the image</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> c channel. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=3065638588&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">The</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> filter <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://ebin.pub/neural-information-processing-30th-international-conference-iconip-2023-changsha-china-november-2023-2023-proceedings-part-v-lecture-notes-in-computer-science-9819980720-9789819980727.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=3065638588&n=3809&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">value</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> of each point constitutes the filter map of that channel. MaxSel splices the filter maps of the three channels to form the filter map Fin∈R3×H×W . X(c,i,j) ? Max(?1,?2,?3,?4) (2) <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 18 in source list: Sangyup Lee, Shahroz Tariq, Youjin Shin, Simon S. Woo. "Detecting handcrafted facial image manipulations and GAN-generated facial images using Shallow-FakeFaceNet", Applied Soft Computing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.asoc.2021.107256', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">As shown in Fig</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">.3, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 18 in source list: Sangyup Lee, Shahroz Tariq, Youjin Shin, Simon S. Woo. "Detecting handcrafted facial image manipulations and GAN-generated facial images using Shallow-FakeFaceNet", Applied Soft Computing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.asoc.2021.107256', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> first <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 18 in source list: Sangyup Lee, Shahroz Tariq, Youjin Shin, Simon S. Woo. "Detecting handcrafted facial image manipulations and GAN-generated facial images using Shallow-FakeFaceNet", Applied Soft Computing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.asoc.2021.107256', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">column is the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> real image <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 18 in source list: Sangyup Lee, Shahroz Tariq, Youjin Shin, Simon S. Woo. "Detecting handcrafted facial image manipulations and GAN-generated facial images using Shallow-FakeFaceNet", Applied Soft Computing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.asoc.2021.107256', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">from the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Wang[21] <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 18 in source list: Sangyup Lee, Shahroz Tariq, Youjin Shin, Simon S. Woo. "Detecting handcrafted facial image manipulations and GAN-generated facial images using Shallow-FakeFaceNet", Applied Soft Computing, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.asoc.2021.107256', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">dataset; the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> second column is the filter map obtained by using the Prewitt operator as the convolution kernel; the third column is the filter map obtained by taking the Laplacian operator as the convolution kernel and the fourth column is the filter map obtained by using MaxSel. Obviously, Tthe filter map obtained by MaxSel is delicate and complete in details, which is favorable for the algorithm to learn more complete features from it. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 4 in source list: https://core.ac.uk/download/227103313.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=3302635314&n=3797&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">Formatted: Font: Not Italic Formatted: Font: Not Italic Field Code Changed Formatted: Font</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">: Not <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 4 in source list: https://core.ac.uk/download/227103313.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=3302635314&n=3797&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">Italic Formatted: Font: Not Italic Formatted: Font: Not Italic Formatted: Font: Not Italic</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, Not Superscript/ Subscript <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 4 in source list: https://core.ac.uk/download/227103313.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=3302635314&n=3797&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">Formatted: Font: Not Italic Formatted: Font: Not Italic Formatted: Font: Not Italic</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Field Code Changed Fig3 Filtering effect image. Each column <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 41 in source list: "Simulation and Synthesis in Medical Imaging", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-68127-6', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">from left to right</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> corresponds to <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 41 in source list: "Simulation and Synthesis in Medical Imaging", Springer Science and Business Media LLC, 2017"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-68127-6', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">the real image</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, Prewitt filtered image, Laplacian filtered image and MaxSel filtered image. 2.2 MResNet Ffeature Eextraction Nnetwork As shown in Fig.4, MResNet is improved from ResNet and has five more MA blocks, which consists of a maximum pooling filter layer, a mean filter layer and a residual layer, than the ResNet. MResNet changes the mean pooling of the final output to maximum pooling, which is used for selecting the maximum features for detecting the generated image. MA block is used to emphasize the local maxima in the feature map as shown in equation (3), where λ is an updatable parameter. Fin denotes the input features. MP denotes maximum pooling. AP denotes mean filtering. Abs denotess taking Computer Applications Research Contributions absolute values. Fout ? MP(Fin) ? ? ? Abs(Fin ? AP(Fin)) (3) Basic <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://ebin.pub/the-international-conference-on-image-vision-and-intelligent-systems-icivis-2021-lecture-notes-in-electrical-engineering-813-9811669627-9789811669620.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=44965040&n=3810&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">block Conv BatchNorm relu</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> maxpool Conv <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://ebin.pub/the-international-conference-on-image-vision-and-intelligent-systems-icivis-2021-lecture-notes-in-electrical-engineering-813-9811669627-9789811669620.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=44965040&n=3810&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">BatchNorm relu Conv</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> BatchNorm Basic <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://ebin.pub/the-international-conference-on-image-vision-and-intelligent-systems-icivis-2021-lecture-notes-in-electrical-engineering-813-9811669627-9789811669620.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=44965040&n=3810&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">block + relu</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Basic <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://ebin.pub/the-international-conference-on-image-vision-and-intelligent-systems-icivis-2021-lecture-notes-in-electrical-engineering-813-9811669627-9789811669620.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=44965040&n=3810&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">block</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Basic block Basic block AvgPool ResNet Conv BatchNorm MA block <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 36 in source list: https://learn.microsoft.com/zh-tw/windows/ai/windows-ml/tutorials/pytorch-train-model"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=699016950&n=3799&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">relu maxpool Conv BatchNorm</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> MA block <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 36 in source list: https://learn.microsoft.com/zh-tw/windows/ai/windows-ml/tutorials/pytorch-train-model"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=699016950&n=3799&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">relu Conv BatchNorm + relu</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Basic block Basic block Basic block MaxPool MResNet X Maxpool Feature M AvgPool Feature A Abs(X-A) + F MA block Fig4 MA block embedded in Basic block in MResNet <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 20 in source list: https://www.scribd.com/document/36462071/Supplementation-of-Nitrogen-Sources-and-Growth-Factors-in-Pineapple-Waste-Extract-Medium-for-Optimum-Yeast-Candida-Utilis-Biomass-Production-1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=4055302953&n=2909&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">Field Code Changed Field Code Changed 2.3</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Classifiers <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 20 in source list: https://www.scribd.com/document/36462071/Supplementation-of-Nitrogen-Sources-and-Growth-Factors-in-Pineapple-Waste-Extract-Medium-for-Optimum-Yeast-Candida-Utilis-Biomass-Production-1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=4055302953&n=2909&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Loss Functions <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 20 in source list: https://www.scribd.com/document/36462071/Supplementation-of-Nitrogen-Sources-and-Growth-Factors-in-Pineapple-Waste-Extract-Medium-for-Optimum-Yeast-Candida-Utilis-Biomass-Production-1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=96.9419845237727&svr=6&lang=zh_hans&sid=4055302953&n=2909&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">The</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 24 in source list: Miaomiao Yu, Jun Zhang, Shuohao Li, Jun Lei. "MSFRNet: Two‐stream deep forgery detector via multi‐scale feature extraction", IET Image Processing, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1049/ipr2.12657', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">classifier C consists of two fully connected layers</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. MaxPix spreads <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 24 in source list: Miaomiao Yu, Jun Zhang, Shuohao Li, Jun Lei. "MSFRNet: Two‐stream deep forgery detector via multi‐scale feature extraction", IET Image Processing, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1049/ipr2.12657', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 8192 features output from MResNet and then transforms them into predicted values using the fully connected layers. As shown in Equation(4), where C is the classifier, y denotes the true label of the images, and Fd isare the input features. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: "Pattern Recognition. ICPR International Workshops and Challenges", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-68780-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">1 N</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Loss ? ? <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: "Pattern Recognition. ICPR International Workshops and Challenges", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-68780-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">N i?1 ? y log</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">(C(Fd )) ? (<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: "Pattern Recognition. ICPR International Workshops and Challenges", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-68780-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">1</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">? y)log(<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: "Pattern Recognition. ICPR International Workshops and Challenges", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-68780-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">1</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">? C(Fd )) (4) but unnamed dataset, referred to as the Wang dataset in this thesis, which is divided into a trainset, an evaluation set, and a testset and contains <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 40 in source list: "Computer Security – ESORICS 2024", Springer Science and Business Media LLC, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-70879-4', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">real images and generated images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. The <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 40 in source list: "Computer Security – ESORICS 2024", Springer Science and Business Media LLC, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-70879-4', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">real images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> were samplied from the LSUN[25] , ImageNet dataset and other datasets that are commonly used to train GANs. The generated images including 20 scenarios images were generated by GANs such as PGGAN, StyleGAN2, and included fake face samplied from the fake face dataset FaceForensics++ (deepfake)[26] . This dataset has been widely used by related Field Code Changed researchers[13,21,27-30] to train and evaluate detection algorithms 3 Experimental This thesis demonstrates the improvement of MaxPix in cross-model generalization performance by comparing the accuracy and average precision of current representative detection algorithms for detecting different datasets. The role of MaxPix modules is verified through ablation experiments. 3.1 Datasets In order to avoid misunderstanding in expression, this thesis uses the lowercase English name of the generativeGAN model to denote the dataset composed of the corresponding <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">generated images and the real images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, e.g., <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 11 in source list: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0290303"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=1040493641&n=3806&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">the images generated by</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> StyleGAN <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 11 in source list: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0290303"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=1040493641&n=3806&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">and the real images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> used for training <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 11 in source list: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0290303"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=1040493641&n=3806&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> generativeGAN model are called the stylegan dataset. The Wang dataset: Wang[21] published a publicly available since its release. The Faces-HQ: Durall[4] released the Faces-HQ dataset. Each image in the Faces-HQ dataset has a resolution of 1024× 1024, which is much better than that of the Wang dataset. Faces-HQ contains 20k real face images which sample from CelebA-HQ[8] and FFHQ, and contains 20k generated images which sample from the 100K Faces project[31] and www.thispersondoesnotexist.com. The generated images are generated by StyleGAN and StyleGAN2. CelebA-HQ and FFHQ are often used to train GANs, which are recognized datasets for training and testing detection algorithms. In this thesis, the person subset of the Wang trainset is used to train PixMSE, and the biggan, gaugan, stargan subsets of the the Wang testset and the Faces-HQ total of more than 102k images are used as testsets. 3.2 Experimental Eenvironment Computer Applications Research Contributions In this thesis, the algorithm code is written with Ppython 3.7 and PyTorch 1.9.0, the GPU used is RTX 3090 and the system used is Ubuntu. MaxPix performs resize as well as random cuts on the trainset and resize as well as center cuts on the testset, which changes the input image into X∈R3×299×299. The training algorithm is set up with the epoch of 36 ,<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-vii-1st-ed-9783030585709-9783030585716.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=257105451&n=3797&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">and the batch-size of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 4. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-vii-1st-ed-9783030585709-9783030585716.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=257105451&n=3797&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">The</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> optimizer is Adam. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-vii-1st-ed-9783030585709-9783030585716.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=257105451&n=3797&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">The</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> learning <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-vii-1st-ed-9783030585709-9783030585716.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=257105451&n=3797&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">rate is 0</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">.00005. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-vii-1st-ed-9783030585709-9783030585716.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=257105451&n=3797&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">The</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> learning <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-vii-1st-ed-9783030585709-9783030585716.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=257105451&n=3797&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">decay</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> rate is <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-vii-1st-ed-9783030585709-9783030585716.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=257105451&n=3797&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">0</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">.96 and the loss function is CrossEntropyLoss. 3.3 Comparative Experiments In this thesis, we select research works that have achieved high accuracy in the task of detecting GAN-generated images in recent years for comparison, including Wang[21], Frank[32], Durall[4], Jeong[13,27], He[22], Deng[33] and Guo[34]. These algorithms not only have achieved better performance in their respective thesis and can achieve more than 90% accuracy in detecting the same type of GAN-generated images, but alsowhile maintaineding a strong cross-model generalization performance. Except for the algorithm of Jeong[13,27] , the rest of the algorithms in this thesis were retrained and tested using the Wang dataset. Since the algorithm of Jeong[13,27] uses the Wang dataset and the code implementation details are not available, the experimental data in the table are quoted from the literature[27]. Tab1 Comparison experiment Wang dataset (%) progan biggan cyclegan deepfake gaugan stargan stylegan stylegan2 Acc AP Wang[21] 81.2 97.9 Frank[32] 98.7 99.9 Durall[4] 66 80.1 He[22] 88.5 99.1 Jeong[13] 82.5 81.4 Jeong[27] 95.5 99.4 Deng[33] 94.2 97.8 Guo[34] 98.6 99.9 MaxPix 98.1 99.9 Acc AP 50.8 67.5 67 89.1 67 73.4 75.9 85.4 67 62.5 63.5 60.5 63.6 68.3 59.3 69.6 82 93.2 Acc AP Acc AP Acc AP 60 86.9 53 61.8 55 88.8 51 69.7 58.9 73.8 65.0 97.6 39.7 42.7 50.3 53.6 64.9 75.2 79.9 88.4 51.7 77.9 50.6 49 75.5 74.2 51.6 49.9 73.6 92.1 59.4 59.9 70.4 81.5 53 49.1 60.6 70.9 77.0 84.5 57.6 60.4 62.4 81.1 59.4 76.8 54.9 67.3 83.5 93.4 69 95.4 63 75.5 Acc AP 56 86 85.8 99.9 69.1 94.6 99.5 100 90.1 90.1 99.6 100 95.4 99.6 98.2 100 100 100 Acc AP 52 76.8 71.3 82.3 75.4 85.8 76.1 90.6 68 62.8 80.6 90.6 85.6 90.8 85.6 92.9 97.2 99.8 Acc AP 52.4 68.3 58.2 71.2 68.3 74.9 59.5 82.6 68.8 63.6 77.4 93.0 92.6 97.3 88.4 96.4 94.5 99.6 Formatted: Font: Not Italic As shown in table 1, MaxPix achieves high accuracy for detecting biggan, cyclegan, stargan, and stylegan datasets, which are higher than the highest values achieved among the compared algorithms. In particular, cCompareding to compared algorithms, MaxPix achieves an accuracy improvement of 6.1% for detecting biggan and 11.6% for detecting stylegan. MaxPix, like most of the compared algorithms, achieves a lower accuracy of 63% for detecting the gaugan dataset. In terms of average precision performance, MaxPix detects gaugan with an average precision of 75.5%, which is lower than the best of the compared algorithms at 97.6%. However, MaxPix detects the remaining seven datasets all get the highest average precision, equaling or exceeding the best of the compared algorithms. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 23 in source list: Lin Cao, Wenjun Sheng, Fan Zhang, Kangning Du, Chong Fu, Peiran Song. "Face Manipulation Detection Based on Supervised Multi-Feature Fusion Attention Network", Sensors, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.3390/s21248181', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">It can be seen that the detection performance of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> MaxPix is better than the current mainstream detection algorithms in terms of accuracy and average precision. Tab2 Comparison experiment Faces-HQ (%) Computer Applications Research Contributions Faces-HQ StyleGAN,CelebA-HQ StyleGAN2,FFHQ ave Acc AP Acc AP Acc AP Wang[21] 49.7 45.1 51.9 74.2 50.8 59.7 Frank[32] 67.2 78.2 58.3 63.6 62.7 70.9 Durall[4] 57.2 93.6 62.9 91 60.0 92.3 He[22] 65.1 85.0 70.2 96.1 67.6 90.6 Deng[33] 79.9 99.2 77.8 93.3 78.9 96.3 Guo[34] 96.4 99.8 82.3 97.8 89.4 98.8 MaxPix 99.9 100 99.3 99.9 99.6 100 As shown in table 2, the average precision and accuracy of the algorithms for detecting the Faces-HQ dataset varyies significantly. Since the implementation details of the Jepong [13,27] algorithm are not available, these two algorithms are not involved in the table2. Despite the fact thatAlthough <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: "Pattern Recognition. ICPR International Workshops and Challenges", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-68780-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">the training and testing are from</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> two <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: "Pattern Recognition. ICPR International Workshops and Challenges", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-68780-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">different</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> datasets <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: "Pattern Recognition. ICPR International Workshops and Challenges", Springer Science and Business Media LLC, 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-030-68780-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">with</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> a huge difference in image resolution and the algorithms are not retrained in this thesis, MaxPix still performs well, obtaining 99.9% and 99.3% detection accuracy and 100% and 99.9% average precision, respectively, which are better than the comparison algorithms. This indicates that MaxPix detection accuracy and average precision are less affected by image size. In addition, in Fig.5, the images in the Wang dataset have obvious artifacts, while the images in Faces-HQ have no obvious artifacts. This also indicates that the detection accuracy of MaxPix is less affected by artifacts. Comparison experiments show that the accuracy and average precision of MaxPix detection algorithms can match or exceed current <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">state-of-the-art</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> detection <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">algorithms</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, with strong <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">cross</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">-model <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">generalization</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> performance. Fig5.<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Caie Xu, Dandan Ni, Bingyan Wang, Mingyang Wu, Honghua Gan. "Two-stage anomaly detection for positive samples and small samples based on generative adversarial networks", Multimedia Tools and Applications, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s11042-022-14306-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">The first row is</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> from <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Caie Xu, Dandan Ni, Bingyan Wang, Mingyang Wu, Honghua Gan. "Two-stage anomaly detection for positive samples and small samples based on generative adversarial networks", Multimedia Tools and Applications, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s11042-022-14306-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> real <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Caie Xu, Dandan Ni, Bingyan Wang, Mingyang Wu, Honghua Gan. "Two-stage anomaly detection for positive samples and small samples based on generative adversarial networks", Multimedia Tools and Applications, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s11042-022-14306-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">image</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> of Faces-HQ, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Caie Xu, Dandan Ni, Bingyan Wang, Mingyang Wu, Honghua Gan. "Two-stage anomaly detection for positive samples and small samples based on generative adversarial networks", Multimedia Tools and Applications, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s11042-022-14306-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">the second row is</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> from <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Caie Xu, Dandan Ni, Bingyan Wang, Mingyang Wu, Honghua Gan. "Two-stage anomaly detection for positive samples and small samples based on generative adversarial networks", Multimedia Tools and Applications, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s11042-022-14306-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">the generated image of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Faces-HQ, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Caie Xu, Dandan Ni, Bingyan Wang, Mingyang Wu, Honghua Gan. "Two-stage anomaly detection for positive samples and small samples based on generative adversarial networks", Multimedia Tools and Applications, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s11042-022-14306-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">and the third row is</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> from <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Caie Xu, Dandan Ni, Bingyan Wang, Mingyang Wu, Honghua Gan. "Two-stage anomaly detection for positive samples and small samples based on generative adversarial networks", Multimedia Tools and Applications, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/s11042-022-14306-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">the generated image of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Wang dataset. 3.4 Ablation Eexperiments This thesis explores the role of MaxSel filtering and MA Block through ablation experiments. The modular ablation experiments use ResNet as a benchmark for comparison. ‘ResNet’ takes the unfiltered image as the input to ResNet. ‘MResNet’ takes the unfiltered image as the input to MResNet , which explores the role of MaxSel. ‘MSel’ filters the image through MaxSel and uses it as input to ResNet to explore the role of MA Block. 3.4.1 Module Aablation Eexperiments As shown in table 3, ResNet only detects stylegan2 and progan with more than 80% accuracy and more than 90% average precision. MResNet does not improve the accuracy and average precision of detecting the generated images despite the addition of MA Block, meaning MA Block alone does not improve the algorithm's performance. Due to the adoption of MaxSel for filtering the image, which makes it easy for the algorithm to learn distinguishable features from the filtered images, thus the detection accuracy and average precision of MSel are comprehensively improved, especially for detecting deepfake, which improves the accuracy by 40.5% and the average precision by 47.9%. MaxPix introduces MA Block on the basis toof Msel to detect progan, biggan, cyclegan, gaugan and stylegan2 with 0.1%, 2.8%, 16.5%, 8.3%, and 0.1% accuracy improvements, respectively. There is a slight decrease in the average precision of MaxPix in detecting deepfake. It can be seen that Maxsel used with MA Block effectively improves the accuracy and average precision of the detection algorithm in detecting the generated images and it is the Maxsel filter that plays the biggest role. Computer Applications Research Contributions Tab3 Module ablation experiment (%) progan biggan cyclegan deepfake gaugan stargan stylegan stylegan2 Acc AP ResNet 85.1 93.4 MResNet 79.1 88.1 MaxSel 98 99.9 MaxPix 98.1 99.9 Acc AP 50.6 55.4 52 56.4 79.2 90.2 82 93.2 Acc AP Acc AP Acc AP 64 70 48.4 48.3 57 65.7 60.7 66.2 48.2 48.3 61 68.5 67 81.5 88.9 96.2 54.7 63.5 83.5 93.4 69 95.4 63 75.5 Acc AP 69.8 80 71.5 86.2 100 100 100 100 Acc AP 67.8 73.5 65.7 72.4 97.4 99.8 97.2 99.8 Acc AP 82.6 90.7 82.9 91.8 94.4 99.4 94.5 99.6 3.4.2 Network Sstructure Aablation Eexperiments In this ablation experiment, filtered images obtained by different filtering algorithms, such as Laplacian, Sobel, Prewitt and Scharr, are used as inputs for MResNet and ResNet to further explore the need for the proposed MaxSel filtering algorithm. Tab4 Network structure ablation experiment-MResNet (%) progan biggan cyclegan deepfake gaugan stargan stylegan stylegan2 Acc AP laplacian 98.1 99.9 prewitt 98.1 99.9 sobel 98.5 99.9 scharr 98.9 99.9 MaxSel 98.1 99.9 Acc AP 79.4 92.6 56 66.8 65.9 75.4 64 70.9 82 93.2 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: Wang, Huaming, Fei, Jianwei, Dai, Yunshu, Leng, Lingyun, Xia, Zhihua. "General GAN-generated image detection by data augmentation in  fingerprint domain", 2023"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=1476659458&n=3793&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">Acc AP Acc AP Acc AP Acc AP</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 83.1 95.6 60.3 76.4 62 77.4 100 100 56.7 76.6 50.6 69.4 51.3 64.4 83.6 99.7 65.4 74.7 80.5 91.3 69.4 78.8 89.5 99.8 67.9 78.5 72.9 85.7 67.7 77.1 94.5 100 83.5 93.4 69 95.4 63 75.5 100 100 Acc AP 94.2 98.6 82.8 94.2 87.4 96.2 86.9 95.5 97.2 99.8 Acc AP 95.9 99.6 84.3 95.5 91.1 97.8 89.5 97.2 94.5 99.6 Tab5 Network structure ablation experiment-ResNet(%) progan biggan cyclegan deepfake gaugan stargan stylegan stylegan2 Acc AP laplacian 96.7 99.8 prewitt sobel scharr MaxSel 97.9 98.8 97.7 98 99.9 99.9 99.8 99.9 Acc AP 78.2 92.2 61.6 72.7 65.8 76.3 68.3 79.7 79.2 90.2 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: Wang, Huaming, Fei, Jianwei, Dai, Yunshu, Leng, Lingyun, Xia, Zhihua. "General GAN-generated image detection by data augmentation in  fingerprint domain", 2023"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=1476659458&n=3793&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">Acc AP Acc AP Acc AP Acc AP</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 67.1 85.4 64.4 80.3 53 58.9 98.5 100 66.8 81 72.1 88.7 70.3 80 87.1 99 66.6 81.6 81.5 88.3 67.8 76 92.6 99.7 76.8 86.7 74.3 85.9 67 76.3 99.6 100 67 81.5 88.9 96.2 54.7 63.5 100 100 Acc AP 93.4 99.3 86.2 86 85.3 97.4 Acc AP 91.3 99.2 94.3 86.5 96.2 90.1 88.7 95.8 90.5 91.4 97.8 99.8 94.4 99.4 As shown in tables 4 and table 5, the detection algorithm uses Maxsel to filter the images and achieves the highest accuracy and average precision on multiple datasets regardless of whether MResNet or ResNet is used as the network architecture. Especially for the detection of stargan, which algorithm consistently achieves 100% accuracy and average precision. The accuracy for the detection of gaugan is consistently lower, at 63% and 54.7%, and the average precision wereas only obtained as 75.5% and 63.5%. However, even when the image is filtered using other operators, the detection algorithm has a low accuracy and average precision for detecting gaugan with maximum accuracy of 70.3% and average precision of only 80%. This indicates that by filtering the image, it is less helpful <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://ebin.pub/the-international-conference-on-image-vision-and-intelligent-systems-icivis-2021-lecture-notes-in-electrical-engineering-813-9811669627-9789811669620.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=44965040&n=3810&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">to improve the accuracy and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> average precision <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://ebin.pub/the-international-conference-on-image-vision-and-intelligent-systems-icivis-2021-lecture-notes-in-electrical-engineering-813-9811669627-9789811669620.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=44965040&n=3810&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">of the algorithm</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> when detecting gaugan. Overall, the two ablation experiments show that Maxsel and MA Block are more helpful in improving <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">the accuracy and average precision</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> of <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> algorithms to detect the GAN-generated Computer Applications Research Contributions images, especially Maxsel filtering can efficiently <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: "Computer Vision – ECCV 2022", Springer Science and Business Media LLC, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-031-19781-9', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">improve the generalization performance of the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> detection algorithms. 4 Conclusion This thesis proposes the MaxPix for detecting GAN-generated images, an algorithm that produces features for detecting generated images by emphasizing the maximum value in the local range of the image. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 21 in source list: "ROBOT 2017: Third Iberian Robotics Conference", Springer Science and Business Media LLC, 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-70833-1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">The main contribution of this</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> thesis <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 21 in source list: "ROBOT 2017: Third Iberian Robotics Conference", Springer Science and Business Media LLC, 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-70833-1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">is to propose</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the MaxSel filtering <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 21 in source list: "ROBOT 2017: Third Iberian Robotics Conference", Springer Science and Business Media LLC, 2018"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-319-70833-1', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">algorithm</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> and the MaxPix detection algorithm. Comparison experiments on Wang and Faces-HQ <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 6 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-iv-1st-ed-9783030585471-9783030585488.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=657331371&n=3796&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">datasets show that</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> MaxPix <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 6 in source list: https://dokumen.pub/computer-vision-eccv-2020-16th-european-conference-glasgow-uk-august-2328-2020-proceedings-part-iv-1st-ed-9783030585471-9783030585488.html"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=657331371&n=3796&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">outperforms the state-of-the-art</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> algorithms such as Deng[33] and Guo[34] in terms of generalization performance, and ablation experiments validate the importance of MaxSel and MA Block in <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 43 in source list: Mingxu Zhang, Hongxia Wang, Peisong He, Asad Malik, Hanqing Liu. "Exposing unseen GAN-generated image using unsupervised domain adaptation", Knowledge-Based Systems, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.knosys.2022.109905', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">improving the detection accuracy</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> and average precision <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 43 in source list: Mingxu Zhang, Hongxia Wang, Peisong He, Asad Malik, Hanqing Liu. "Exposing unseen GAN-generated image using unsupervised domain adaptation", Knowledge-Based Systems, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.knosys.2022.109905', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">of the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> detection algorithms. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 43 in source list: Mingxu Zhang, Hongxia Wang, Peisong He, Asad Malik, Hanqing Liu. "Exposing unseen GAN-generated image using unsupervised domain adaptation", Knowledge-Based Systems, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.knosys.2022.109905', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">The</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> research in this thesis provides a reference <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: Wang, Huaming, Fei, Jianwei, Dai, Yunshu, Leng, Lingyun, Xia, Zhihua. "General GAN-generated image detection by data augmentation in  fingerprint domain", 2023"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=1476659458&n=3793&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">for the detection of GAN-generated images</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. 5 References [1] Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial networks[J]. Communications of the ACM, 2020, 63(11): 139-144. [2] Zhang Y. Research on Deepfake detection method based on deep learning [D]. Yunnan University, 2022. [3] McCloskey S, Albright M. Detecting GAN-generated imagery using saturation cues[C]//2019 IEEE international conference on image processing (ICIP). ieee, 2019: 4584-4588. [4] Durall R, Keuper M, Keuper J. Watch your up-convolution: cnn based generative deep neural networks are failing to reproduce spectral distributions[C]/ /Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 7890-7899. [5] Guo H, Hu S, Wang X, et al. Eyes tell all: Irregular pupil shapes reveal GAN-generated faces[C]//2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022: 2904-2908. [6] Yu J, Jiang Y, Wang Z, et al. Unitbox: an advanced object detection network[C]//Proceedings of the 24th ACM international conference on Multimedia. 2016 : 516-520. [7] Liu Y, Wan Z, Yin X, et al. Detection of GAN generated image using color gradient representation[J]. Journal of Visual Communication and Image Representation, 2023, 95: 103876. [8] Karras T , Aila T , Laine S ,et al. Progressive Growing of GANs for Improved Quality, Stability, and Variation[J]. 2017. doi:10.48550/arXiv.1710.10196. [9] Zhang X, Karaman S, Chang S F. Detecting and simulating artifacts in GAN fake images[C]//2019 IEEE international workshop on information forensics and security (WIFS). ieee, 2019: 1-6. [10] Liu H, Li X, Zhou W, et al. Spatial-phase shallow learning: rethinking face forgery detection in frequency domain[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021: 772-781. [11] Rossler A, Cozzolino D, Verdoliva L, et al. Faceforensics++: Learning to detect manipulated facial images[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 1-11. [12] Li Y, Yang X, Sun P, et al. Celeb-df: A large-scale challenging dataset for deepfake forensics[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 3207-3216. [13] Jeong Y, Kim D, Min S, et al. Bihpf: Bilateral high-pass filters for robust deepfake detection[C]//Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022: 48-57. [14] Tian C, Luo Z, Shi G, et al. Frequency-aware attentional feature fusion for deepfake detection[C]//2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5. [15] Wang B, Wu X, Tang Y, et al. Frequency domain filtered residual network for deepfake detection[J]. Mathematics, 2023, 11(4): 816. [16] Chollet F .Xception: Deep Learning with Depthwise Separable Convolutions[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR).IEEE, 2017.DOI:10.1109/CVPR.2017.195. [17] Miao C, Tan Z, Chu Q, et al. F 2 trans: high-frequency fine-grained transformer for face forgery detection[J]. IEEE Transactions on Information Forensics and Security, 2023, 18: 1039-1051. [18] Choi Y , Choi M , Kim M ,et al. StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation[C]//IEEE/CVF Conference on Computer Vision and Pattern Recognition.0[2024-10-02].DOI:10.48550/arXiv.1711.09020. [19] Karras T, Laine S, Aittala M, et al. Analyzing and improving the image quality of stylegan[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern Formatted: Font: (Default) SimHei Computer Applications Research Contributions recognition. 2020: 8110-8119. [20] Karras T, Laine S, Aila T. A style-based generator architecture for generative adversarial networks[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 4401-4410. [21] Wang S Y, Wang O, Zhang R, et al. CNN-generated images are surprisingly easy to spot... for now[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 8695-8704. [22] He Y, Yu N, Keuper M, et al. Beyond the spectrum: Detecting deepfakes via re-synthesis [DB]. arxiv preprint arxiv:2105.14376, 2021. [23] Brock A , Donahue J , Simonyan K .Large Scale GAN Training for High Fidelity Natural Image Synthesis[J]. 2018. doi:10.48550/arXiv.1809.11096. [24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,Aditya Khosla, Michael Bernstein, et al . Imagenet large scale visual recognition challenge. ijcv, 2015. 3 [25] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. lsun: Construction of a large-scale image dataset using deep learning with humans in the loop [DB]. arXiv preprint arXiv:1506.03365, 2015. 3, 4 [26] Rossler A, Cozzolino D, Verdoliva L, et al. Faceforensics++: Learning to detect manipulated facial images[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 1-11. [27] Jeong Y, Kim D, Ro Y, et al. FrePGAN: robust deepfake detection using frequency-level perturbations[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2022, 36(1): 1060-1068. [28] Tanaka M, Shiota S, Kiya H. A universal detector of CNN-generated images using properties of checkerboard artifacts in the frequency domain[C]//2021 IEEE 10th Global Conference on Consumer Electronics (GCCE). IEEE, 2021: 103-106. [29] Dong C, Kumar A, Liu E. Think twice before detecting gan-generated fake images from their spectral domain imprints[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 7865-7874. [30] Arruda P H R. Synthetic image detection using a modern CNN and noise patterns[D]. , 2023. [31] 100000 faces generated. https://generated.photos/. [32] Frank J, Eisenhofer T, Schönherr L, et al. Leveraging frequency analysis for deep fake image recognition[C]//International conference on machine learning. pmlr, 2020: 3247-3258. [33] Deng X, Zhao B, Guan Z, et al. New finding and unified framework for fake image detection[J]. IEEE Signal Processing Letters, 2023, 30: 90-94. [34] Guo Z, Yang G, Zhang D, et al. Rethinking gradient operator for exposing AI-enabled face forgeries[J]. Expert Systems with Applications, 2023, 215: 119361. Field Code Changed <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 38 in source list: https://WWW.coursehero.com/file/39105282/2-Joshua-Rothman-Why-Is-Academic-Writing-So-Academicpdf/"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=90.2300703914449&svr=6&lang=zh_hans&sid=681731563&n=3808&svr=6&session-id=7a4dab44b05b44e78767a1c49d1fb050', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">Contributions Contributions Contributions Contributions Contributions Contributions Contributions Contributions</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Contributions </p></div></body></html>
